g nr_stack_trace_entries;

#ifdef CONFIG_PROVE_LOCKING
/**
 * struct lock_trace - single stack backtrace
 * @hash_entry:	Entry in a stack_trace_hash[] list.
 * @hash:	jhash() of @entries.
 * @nr_entries:	Number of entries in @entries.
 * @entries:	Actual stack backtrace.
 */
struct lock_trace {
	struct hlist_node	hash_entry;
	u32			hash;
	u32			nr_entries;
	unsigned long		entries[] __aligned(sizeof(unsigned long));
};
#define LOCK_TRACE_SIZE_IN_LONGS				\
	(sizeof(struct lock_trace) / sizeof(unsigned long))
/*
 * Stack-trace: sequence of lock_trace structures. Protected by the graph_lock.
 */
static unsigned long stack_trace[MAX_STACK_TRACE_ENTRIES];
static struct hlist_head stack_trace_hash[STACK_TRACE_HASH_SIZE];

static bool traces_identical(struct lock_trace *t1, struct lock_trace *t2)
{
	return t1->hash == t2->hash && t1->nr_entries == t2->nr_entries &&
		memcmp(t1->entries, t2->entries,
		       t1->nr_entries * sizeof(t1->entries[0])) == 0;
}

static struct lock_trace *save_trace(void)
{
	struct lock_trace *trace, *t2;
	struct hlist_head *hash_head;
	u32 hash;
	int max_entries;

	BUILD_BUG_ON_NOT_POWER_OF_2(STACK_TRACE_HASH_SIZE);
	BUILD_BUG_ON(LOCK_TRACE_SIZE_IN_LONGS >= MAX_STACK_TRACE_ENTRIES);

	trace = (struct lock_trace *)(stack_trace + nr_stack_trace_entries);
	max_entries = MAX_STACK_TRACE_ENTRIES - nr_stack_trace_entries -
		LOCK_TRACE_SIZE_IN_LONGS;

	if (max_entries <= 0) {
		if (!debug_locks_off_graph_unlock())
			return NULL;

		print_lockdep_off("BUG: MAX_STACK_TRACE_ENTRIES too low!");
		dump_stack();

		return NULL;
	}
	trace->nr_entries = stack_trace_save(trace->entries, max_entries, 3);

	hash = jhash(trace->entries, trace->nr_entries *
		     sizeof(trace->entries[0]), 0);
	trace->hash = hash;
	hash_head = stack_trace_hash + (hash & (STACK_TRACE_HASH_SIZE - 1));
	hlist_for_each_entry(t2, hash_head, hash_entry) {
		if (traces_identical(trace, t2))
			return t2;
	}
	nr_stack_trace_entries += LOCK_TRACE_SIZE_IN_LONGS + trace->nr_entries;
	hlist_add_head(&trace->hash_entry, hash_head);

	return trace;
}

/* Return the number of stack traces in the stack_trace[] array. */
u64 lockdep_stack_trace_count(void)
{
	struct lock_trace *trace;
	u64 c = 0;
	int i;

	for (i = 0; i < ARRAY_SIZE(stack_trace_hash); i++) {
		hlist_for_each_entry(trace, &stack_trace_hash[i], hash_entry) {
			c++;
		}
	}

	return c;
}

/* Return the number of stack hash chains that have at least one stack trace. */
u64 lockdep_stack_hash_count(void)
{
	u64 c = 0;
	int i;

	for (i = 0; i < ARRAY_SIZE(stack_trace_hash); i++)
		if (!hlist_empty(&stack_trace_hash[i]))
			c++;

	return c;
}
#endif

unsigned int nr_hardirq_chains;
unsigned int nr_softirq_chains;
unsigned int nr_process_chains;
unsigned int max_lockdep_depth;

#ifdef CONFIG_DEBUG_LOCKDEP
/*
 * Various lockdep statistics:
 */
DEFINE_PER_CPU(struct lockdep_stats, lockdep_stats);
#endif

#ifdef CONFIG_PROVE_LOCKING
/*
 * Locking printouts:
 */

#define __USAGE(__STATE)						\
	[LOCK_USED_IN_##__STATE] = "IN-"__stringify(__STATE)"-W",	\
	[LOCK_ENABLED_##__STATE] = __stringify(__STATE)"-ON-W",		\
	[LOCK_USED_IN_##__STATE##_READ] = "IN-"__stringify(__STATE)"-R",\
	[LOCK_ENABLED_##__STATE##_READ] = __stringify(__STATE)"-ON-R",

static const char *usage_str[] =
{
#define LOCKDEP_STATE(__STATE) __USAGE(__STATE)
#include "lockdep_states.h"
#undef LOCKDEP_STATE
	[LOCK_USED] = "INITIAL USE",
	[LOCK_USED_READ] = "INITIAL READ USE",
	/* abused as string storage for verify_lock_unused() */
	[LOCK_USAGE_STATES] = "IN-NMI",
};
#endif

const char *__get_key_name(const struct lockdep_subclass_key *key, char *str)
{
	return kallsyms_lookup((unsigned long)key, NULL, NULL, NULL, str);
}

static inline unsigned long lock_flag(enum lock_usage_bit bit)
{
	return 1UL << bit;
}

static char get_usage_char(struct lock_class *class, enum lock_usage_bit bit)
{
	/*
	 * The usage character defaults to '.' (i.e., irqs disabled and not in
	 * irq context), which is the safest usage category.
	 */
	char c = '.';

	/*
	 * The order of the following usage checks matters, which will
	 * result in the outcome character as follows:
	 *
	 * - '+': irq is enabled and not in irq context
	 * - '-': in irq context and irq is disabled
	 * - '?': in irq context and irq is enabled
	 */
	if (class->usage_mask & lock_flag(bit + LOCK_USAGE_DIR_MASK)) {
		c = '+';
		if (class->usage_mask & lock_flag(bit))
			c = '?';
	} else if (class->usage_mask & lock_flag(bit))
		c = '-';

	return c;
}

void get_usage_chars(struct lock_class *class, char usage[LOCK_USAGE_CHARS])
{
	int i = 0;

#define LOCKDEP_STATE(__STATE) 						\
	usage[i++] = get_usage_char(class, LOCK_USED_IN_##__STATE);	\
	usage[i++] = get_usage_char(class, LOCK_USED_IN_##__STATE##_READ);
#include "lockdep_states.h"
#undef LOCKDEP_STATE

	usage[i] = '\0';
}

static void __print_lock_name(struct lock_class *class)
{
	char str[KSYM_NAME_LEN];
	const char *name;

	name = class->name;
	if (!name) {
		name = __get_key_name(class->key, str);
		printk(KERN_CONT "%s", name);
	} else {
		printk(KERN_CONT "%s", name);
		if (class->name_version > 1)
			printk(KERN_CONT "#%d", class->name_version);
		if (class->subclass)
			printk(KERN_CONT "/%d", class->subclass);
	}
}

static void print_lock_name(struct lock_class *class)
{
	char usage[LOCK_USAGE_CHARS];

	get_usage_chars(class, usage);

	printk(KERN_CONT " (");
	__print_lock_name(class);
	printk(KERN_CONT "){%s}-{%d:%d}", usage,
			class->wait_type_outer ?: class->wait_type_inner,
			class->wait_type_inner);
}

static void print_lockdep_cache(struct lockdep_map *lock)
{
	const char *name;
	char str[KSYM_NAME_LEN];

	name = lock->name;
	if (!name)
		name = __get_key_name(lock->key->subkeys, str);

	printk(KERN_CONT "%s", name);
}

static void print_lock(struct held_lock *hlock)
{
	/*
	 * We can be called locklessly through debug_show_all_locks() so be
	 * extra careful, the hlock might have been released and cleared.
	 *
	 * If this indeed happens, lets pretend it does not hurt to continue
	 * to print the lock unless the hlock class_idx does not point to a
	 * registered class. The rationale here is: since we don't attempt
	 * to distinguish whether we are in this situation, if it just
	 * happened we can't count on class_idx to tell either.
	 */
	struct lock_class *lock = hlock_class(hlock);

	if (!lock) {
		printk(KERN_CONT "<RELEASED>\n");
		return;
	}

	printk(KERN_CONT "%px", hlock->instance);
	print_lock_name(lock);
	printk(KERN_CONT ", at: %pS\n", (void *)hlock->acquire_ip);
}

static void lockdep_print_held_locks(struct task_struct *p)
{
	int i, depth = READ_ONCE(p->lockdep_depth);

	if (!depth)
		printk("no locks held by %s/%d.\n", p->comm, task_pid_nr(p));
	else
		printk("%d lock%s held by %s/%d:\n", depth,
		       depth > 1 ? "s" : "", p->comm, task_pid_nr(p));
	/*
	 * It's not reliable to print a task's held locks if it's not sleeping
	 * and it's not the current task.
	 */
	if (p != current && task_is_running(p))
		return;
	for (i = 0; i < depth; i++) {
		printk(" #%d: ", i);
		print_lock(p->held_locks + i);
	}
}

static void print_kernel_ident(void)
{
	printk("%s %.*s %s\n", init_utsname()->release,
		(int)strcspn(init_utsname()->version, " "),
		init_utsname()->version,
		print_tainted());
}

static int very_verbose(struct lock_class *class)
{
#if VERY_VERBOSE
	return class_filter(class);
#endif
	return 0;
}

/*
 * Is this the address of a static object:
 */
#ifdef __KERNEL__
/*
 * Check if an address is part of freed initmem. After initmem is freed,
 * memory can be allocated from it, and such allocations would then have
 * addresses within the range [_stext, _end].
 */
#ifndef arch_is_kernel_initmem_freed
static int arch_is_kernel_initmem_freed(unsigned long addr)
{
	if (system_state < SYSTEM_FREEING_INITMEM)
		return 0;

	return init_section_contains((void *)addr, 1);
}
#endif

static int static_obj(const void *obj)
{
	unsigned long start = (unsigned long) &_stext,
		      end   = (unsigned long) &_end,
		      addr  = (unsigned long) obj;

	if (arch_is_kernel_initmem_freed(addr))
		return 0;

	/*
	 * static variable?
	 */
	if ((addr >= start) && (addr < end))
		return 1;

	/*
	 * in-kernel percpu var?
	 */
	if (is_kernel_percpu_address(addr))
		return 1;

	/*
	 * module static or percpu var?
	 */
	return is_module_address(addr) || is_module_percpu_address(addr);
}
#endif

/*
 * To make lock name printouts unique, we calculate a unique
 * class->name_version generation counter. The caller must hold the graph
 * lock.
 */
static int count_matching_names(struct lock_class *new_class)
{
	struct lock_class *class;
	int count = 0;

	if (!new_class->name)
		return 0;

	list_for_each_entry(class, &all_lock_classes, lock_entry) {
		if (new_class->key - new_class->subclass == class->key)
			return class->name_version;
		if (class->name && !strcmp(class->name, new_class->name))
			count = max(count, class->name_version);
	}

	return count + 1;
}

/* used from NMI context -- must be lockless */
static noinstr struct lock_class *
look_up_lock_class(const struct lockdep_map *lock, unsigned int subclass)
{
	struct lockdep_subclass_key *key;
	struct hlist_head *hash_head;
	struct lock_class *class;

	if (unlikely(subclass >= MAX_LOCKDEP_SUBCLASSES)) {
		instrumentation_begin();
		debug_locks_off();
		printk(KERN_ERR
			"BUG: looking up invalid subclass: %u\n", subclass);
		printk(KERN_ERR
			"turning off the locking correctness validator.\n");
		dump_stack();
		instrumentation_end();
		return NULL;
	}

	/*
	 * If it is not initialised then it has never been locked,
	 * so it won't be present in the hash table.
	 */
	if (unlikely(!lock->key))
		return NULL;

	/*
	 * NOTE: the class-key must be unique. For dynamic locks, a static
	 * lock_class_key variable is passed in through the mutex_init()
	 * (or spin_lock_init()) call - which acts as the key. For static
	 * locks we use the lock object itself as the key.
	 */
	BUILD_BUG_ON(sizeof(struct lock_class_key) >
			sizeof(struct lockdep_map));

	key = lock->key->subkeys + subclass;

	hash_head = classhashentry(key);

	/*
	 * We do an RCU walk of the hash, see lockdep_free_key_range().
	 */
	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
		return NULL;

	hlist_for_each_entry_rcu_notrace(class, hash_head, hash_entry) {
		if (class->key == key) {
			/*
			 * Huh! same key, different name? Did someone trample
			 * on some memory? We're most confused.
			 */
			WARN_ON_ONCE(class->name != lock->name &&
				     lock->key != &__lockdep_no_validate__);
			return class;
		}
	}

	return NULL;
}

/*
 * Static locks do not have their class-keys yet - for them the key is
 * the lock object itself. If the lock is in the per cpu area, the
 * canonical address of the lock (per cpu offset removed) is used.
 */
static bool assign_lock_key(struct lockdep_map *lock)
{
	unsigned long can_addr, addr = (unsigned long)lock;

#ifdef __KERNEL__
	/*
	 * lockdep_free_key_range() assumes that struct lock_class_key
	 * objects do not overlap. Since we use the address of lock
	 * objects as class key for static objects, check whether the
	 * size of lock_class_key objects does not exceed the size of
	 * the smallest lock object.
	 */
	BUILD_BUG_ON(sizeof(struct lock_class_key) > sizeof(raw_spinlock_t));
#endif

	if (__is_kernel_percpu_address(addr, &can_addr))
		lock->key = (void *)can_addr;
	else if (__is_module_percpu_address(addr, &can_addr))
		lock->key = (void *)can_addr;
	else if (static_obj(lock))
		lock->key = (void *)lock;
	else {
		/* Debug-check: all keys must be persistent! */
		debug_locks_off();
		pr_err("INFO: trying to register non-static key.\n");
		pr_err("The code is fine but needs lockdep annotation, or maybe\n");
		pr_err("you didn't initialize this object before use?\n");
		pr_err("turning off the locking correctness validator.\n");
		dump_stack();
		return false;
	}

	return true;
}

#ifdef CONFIG_DEBUG_LOCKDEP

/* Check whether element @e occurs in list @h */
static bool in_list(struct list_head *e, struct list_head *h)
{
	struct list_head *f;

	list_for_each(f, h) {
		if (e == f)
			return true;
	}

	return false;
}

/*
 * Check whether entry @e occurs in any of the locks_after or locks_before
 * lists.
 */
static bool in_any_class_list(struct list_head *e)
{
	struct lock_class *class;
	int i;

	for (i = 0; i < A_do_set_cpus_allowed(p, cpumask_of(rq->cpu), SCA_MIGRATE_DISABLE);
}

void migrate_disable(void)
{
	struct task_struct *p = current;

	if (p->migration_disabled) {
		p->migration_disabled++;
		return;
	}

	preempt_disable();
	this_rq()->nr_pinned++;
	p->migration_disabled = 1;
	preempt_enable();
}
EXPORT_SYMBOL_GPL(migrate_disable);

void migrate_enable(void)
{
	struct task_struct *p = current;

	if (p->migration_disabled > 1) {
		p->migration_disabled--;
		return;
	}

	if (WARN_ON_ONCE(!p->migration_disabled))
		return;

	/*
	 * Ensure stop_task runs either before or after this, and that
	 * __set_cpus_allowed_ptr(SCA_MIGRATE_ENABLE) doesn't schedule().
	 */
	preempt_disable();
	if (p->cpus_ptr != &p->cpus_mask)
		__set_cpus_allowed_ptr(p, &p->cpus_mask, SCA_MIGRATE_ENABLE);
	/*
	 * Mustn't clear migration_disabled() until cpus_ptr points back at the
	 * regular cpus_mask, otherwise things that race (eg.
	 * select_fallback_rq) get confused.
	 */
	barrier();
	p->migration_disabled = 0;
	this_rq()->nr_pinned--;
	preempt_enable();
}
EXPORT_SYMBOL_GPL(migrate_enable);

static inline bool rq_has_pinned_tasks(struct rq *rq)
{
	return rq->nr_pinned;
}

/*
 * Per-CPU kthreads are allowed to run on !active && online CPUs, see
 * __set_cpus_allowed_ptr() and select_fallback_rq().
 */
static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
{
	/* When not in the task's cpumask, no point in looking further. */
	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
		return false;

	/* migrate_disabled() must be allowed to finish. */
	if (is_migration_disabled(p))
		return cpu_online(cpu);

	/* Non kernel threads are not allowed during either online or offline. */
	if (!(p->flags & PF_KTHREAD))
		return cpu_active(cpu) && task_cpu_possible(cpu, p);

	/* KTHREAD_IS_PER_CPU is always allowed. */
	if (kthread_is_per_cpu(p))
		return cpu_online(cpu);

	/* Regular kernel threads don't get to stay during offline. */
	if (cpu_dying(cpu))
		return false;

	/* But are allowed during online. */
	return cpu_online(cpu);
}

/*
 * This is how migration works:
 *
 * 1) we invoke migration_cpu_stop() on the target CPU using
 *    stop_one_cpu().
 * 2) stopper starts to run (implicitly forcing the migrated thread
 *    off the CPU)
 * 3) it checks whether the migrated task is still in the wrong runqueue.
 * 4) if it's in the wrong runqueue then the migration thread removes
 *    it and puts it into the right queue.
 * 5) stopper completes and stop_one_cpu() returns and the migration
 *    is done.
 */

/*
 * move_queued_task - move a queued task to new rq.
 *
 * Returns (locked) new rq. Old rq's lock is released.
 */
static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,
				   struct task_struct *p, int new_cpu)
{
	lockdep_assert_rq_held(rq);

	deactivate_task(rq, p, DEQUEUE_NOCLOCK);
	set_task_cpu(p, new_cpu);
	rq_unlock(rq, rf);

	rq = cpu_rq(new_cpu);

	rq_lock(rq, rf);
	BUG_ON(task_cpu(p) != new_cpu);
	activate_task(rq, p, 0);
	check_preempt_curr(rq, p, 0);

	return rq;
}

struct migration_arg {
	struct task_struct		*task;
	int				dest_cpu;
	struct set_affinity_pending	*pending;
};

/*
 * @refs: number of wait_for_completion()
 * @stop_pending: is @stop_work in use
 */
struct set_affinity_pending {
	refcount_t		refs;
	unsigned int		stop_pending;
	struct completion	done;
	struct cpu_stop_work	stop_work;
	struct migration_arg	arg;
};

/*
 * Move (not current) task off this CPU, onto the destination CPU. We're doing
 * this because either it can't run here any more (set_cpus_allowed()
 * away from this CPU, or CPU going down), or because we're
 * attempting to rebalance this task on exec (sched_exec).
 *
 * So we race with normal scheduler movements, but that's OK, as long
 * as the task is no longer on this CPU.
 */
static struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf,
				 struct task_struct *p, int dest_cpu)
{
	/* Affinity changed (again). */
	if (!is_cpu_allowed(p, dest_cpu))
		return rq;

	update_rq_clock(rq);
	rq = move_queued_task(rq, rf, p, dest_cpu);

	return rq;
}

/*
 * migration_cpu_stop - this will be executed by a highprio stopper thread
 * and performs thread migration by bumping thread off CPU then
 * 'pushing' onto another runqueue.
 */
static int migration_cpu_stop(void *data)
{
	struct migration_arg *arg = data;
	struct set_affinity_pending *pending = arg->pending;
	struct task_struct *p = arg->task;
	struct rq *rq = this_rq();
	bool complete = false;
	struct rq_flags rf;

	/*
	 * The original target CPU might have gone down and we might
	 * be on another CPU but it doesn't matter.
	 */
	local_irq_save(rf.flags);
	/*
	 * We need to explicitly wake pending tasks before running
	 * __migrate_task() such that we will not miss enforcing cpus_ptr
	 * during wakeups, see set_cpus_allowed_ptr()'s TASK_WAKING test.
	 */
	flush_smp_call_function_from_idle();

	raw_spin_lock(&p->pi_lock);
	rq_lock(rq, &rf);

	/*
	 * If we were passed a pending, then ->stop_pending was set, thus
	 * p->migration_pending must have remained stable.
	 */
	WARN_ON_ONCE(pending && pending != p->migration_pending);

	/*
	 * If task_rq(p) != rq, it cannot be migrated here, because we're
	 * holding rq->lock, if p->on_rq == 0 it cannot get enqueued because
	 * we're holding p->pi_lock.
	 */
	if (task_rq(p) == rq) {
		if (is_migration_disabled(p))
			goto out;

		if (pending) {
			p->migration_pending = NULL;
			complete = true;

			if (cpumask_test_cpu(task_cpu(p), &p->cpus_mask))
				goto out;
		}

		if (task_on_rq_queued(p))
			rq = __migrate_task(rq, &rf, p, arg->dest_cpu);
		else
			p->wake_cpu = arg->dest_cpu;

		/*
		 * XXX __migrate_task() can fail, at which point we might end
		 * up running on a dodgy CPU, AFAICT this can only happen
		 * during CPU hotplug, at which point we'll get pushed out
		 * anyway, so it's probably not a big deal.
		 */

	} else if (pending) {
		/*
		 * This happens when we get migrated between migrate_enable()'s
		 * preempt_enable() and scheduling the stopper task. At that
		 * point we're a regular task again and not current anymore.
		 *
		 * A !PREEMPT kernel has a giant hole here, which makes it far
		 * more likely.
		 */

		/*
		 * The task moved before the stopper got to run. We're holding
		 * ->pi_lock, so the allowed mask is stable - if it got
		 * somewhere allowed, we're done.
		 */
		if (cpumask_test_cpu(task_cpu(p), p->cpus_ptr)) {
			p->migration_pending = NULL;
			complete = true;
			goto out;
		}

		/*
		 * When migrate_enable() hits a rq mis-match we can't reliably
		 * determine is_migration_disabled() and so have to chase after
		 * it.
		 */
		WARN_ON_ONCE(!pending->stop_pending);
		task_rq_unlock(rq, p, &rf);
		stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
				    &pending->arg, &pending->stop_work);
		return 0;
	}
out:
	if (pending)
		pending->stop_pending = false;
	task_rq_unlock(rq, p, &rf);

	if (complete)
		complete_all(&pending->done);

	return 0;
}

int push_cpu_stop(void *arg)
{
	struct rq *lowest_rq = NULL, *rq = this_rq();
	struct task_struct *p = arg;

	raw_spin_lock_irq(&p->pi_lock);
	raw_spin_rq_lock(rq);

	if (task_rq(p) != rq)
		goto out_unlock;

	if (is_migration_disabled(p)) {
		p->migration_flags |= MDF_PUSH;
		goto out_unlock;
	}

	p->migration_flags &= ~MDF_PUSH;

	if (p->sched_class->find_lock_rq)
		lowest_rq = p->sched_class->find_lock_rq(p, rq);

	if (!lowest_rq)
		goto out_unlock;

	// XXX validate p is still the highest prio task
	if (task_rq(p) == rq) {
		deactivate_task(rq, p, 0);
		set_task_cpu(p, lowest_rq->cpu);
		activate_task(lowest_rq, p, 0);
		resched_curr(lowest_rq);
	}

	double_unlock_balance(rq, lowest_rq);

out_unlock:
	rq->push_busy = false;
	raw_spin_rq_unlock(rq);
	raw_spin_unlock_irq(&p->pi_lock);

	put_task_struct(p);
	return 0;
}

/*
 * sched_class::set_cpus_allowed must do the below, but is not required to
 * actually call this function.
 */
void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
{
	if (flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
		p->cpus_ptr = new_mask;
		return;
	}

	cpumask_copy(&p->cpus_mask, new_mask);
	p->nr_cpus_allowed = cpumask_weight(new_mask);
}

static void
__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
{
	struct rq *rq = task_rq(p);
	bool queued, running;

	/*
	 * This here violates the locking rules for affinity, since we're only
	 * supposed to change these variables while holding both rq->lock and
	 * p->pi_lock.
	 *
	 * HOWEVER, it magically works, because ttwu() is the only code that
	 * accesses these variables under p->pi_lock and only does so after
	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule()
	 * before finish_task().
	 *
	 * XXX do further audits, this smells like something putrid.
	 */
	if (flags & SCA_MIGRATE_DISABLE)
		SCHED_WARN_ON(!p->on_cpu);
	else
		lockdep_assert_held(&p->pi_lock);

	queued = task_on_rq_queued(p);
	running = task_current(rq, p);

	if (queued) {
		/*
		 * Because __kthread_bind() calls this on blocked tasks without
		 * holding rq->lock.
		 */
		lockdep_assert_rq_held(rq);
		dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
	}
	if (running)
		put_prev_task(rq, p);

	p->sched_class->set_cpus_allowed(p, new_mask, flags);

	if (queued)
		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
	if (running)
		set_next_task(rq, p);
}

void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
{
	__do_set_cpus_allowed(p, new_mask, 0);
}

int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
		      int node)
{
	if (!src->user_cpus_ptr)
		return 0;

	dst->user_cpus_ptr = kmalloc_node(cpumask_size(), GFP_KERNEL, node);
	if (!dst->user_cpus_ptr)
		return -ENOMEM;

	cpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);
	return 0;
}

static inline struct cpumask *clear_user_cpus_ptr(struct task_struct *p)
{
	struct cpumask *user_mask = NULL;

	swap(p->user_cpus_ptr, user_mask);

	return user_mask;
}

void release_user_cpus_ptr(struct task_struct *p)
{
	kfree(clear_user_cpus_ptr(p));
}

/*
 * This function is wildly self concurrent; here be dragons.
 *
 *
 * When given a valid mask, __set_cpus_allowed_ptr() must block until the
 * designated task is enqueued on an allowed CPU. If that task is currently
 * running, we have to kick it out using the CPU stopper.
 *
 * Migrate-Disable comes along and tramples all over our nice sandcastle.
 * Consider:
 *
 *     Initial conditions: P0->cpus_mask = [0, 1]
 *
 *     P0@CPU0                  P1
 *
 *     migrate_disable();
 *     <preempted>
 *                              set_cpus_allowed_ptr(P0, [1]);
 *
 * P1 *cannot* return from this set_cpus_allowed_ptr() call until P0 executes
 * its outermost migrate_enable() (i.e. it exits its Migrate-Disable region).
 * This means we need the following scheme:
 *
 *     P0@CPU0                  P1
 *
 *     migrate_disable();
 *     <preempted>
 *                              set_cpus_allowed_ptr(P0, [1]);
 *                                <blocks>
 *     <resumes>
 *     migrate_enable();
 *       __set_cpus_allowed_ptr();
 *       <wakes local stopper>
 *                         `--> <woken on migration completion>
 *
 * Now the fun stuff: there may be several P1-like tasks, i.e. multiple
 * concurrent set_cpus_allowed_ptr(P0, [*]) calls. CPU affinity changes of any
 * task p are serialized by p->pi_lock, which we can leverage: the one that
 * should come into effect at the end of the Migrate-Disable region is the last
 * one. This means we only need to track a single cpumask (i.e. p->cpus_mask),
 * but we still need to properly signal those waiting tasks at the appropriate
 * moment.
 *
 * This is implemented using struct set_affinity_pending. The first
 * __set_cpus_allowed_ptr() caller within a given Migrate-Disable region will
 * setup an instance of that struct and install it on the targeted task_struct.
 * Any and all further callers will reuse that instance. Those then wait for
 * a completion signaled at the tail of the CPU stopper callback (1), triggered
 * on the end of the Migrate-Disable region (i.e. outermost migrate_enable()).
 *
 *
 * (1) In the cases covered above. There is one more where the completion is
 * signaled within affine_move_task() itself: when a subsequent affinity request
 * occurs after the stopper bailed out due to the targeted task still being
 * Migrate-Disable. Consider:
 *
 *     Initial conditions: P0->cpus_mask = [0, 1]
 *
 *     CPU0		  P1				P2
 *     <P0>
 *       migrate_disable();
 *       <preempted>
 *                        set_cpus_allowed_ptr(P0, [1]);
 *                          <blocks>
 *     <migration/0>
 *       migration_cpu_stop()
 *         is_migration_disabled()
 *           <bails>
 *                                                       set_cpus_allowed_ptr(P0, [0, 1]);
 *                                                         <signal completion>
 *                          <awakes>
 *
 * Note that the above is safe vs a concurrent migrate_enable(), as any
 * pending affinity completion is preceded by an uninstallation of
 * p->migration_pending done with p->pi_lock held.
 */
static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,
			    int dest_cpu, unsigned int flags)
{
	struct set_affinity_pending my_pending = { }, *pending = NULL;
	bool stop_pending, complete = false;

	/* Can the task run on the task's current CPU? If so, we're done */
	if (cpumask_test_cpu(task_cpu(p), &p->cpus_mask)) {
		struct task_struct *push_task = NULL;

		if ((flags & SCA_MIGRATE_ENABLE) &&
		    (p->migration_flags & MDF_PUSH) && !rq->push_busy) {
			rq->push_busy = true;
			push_task = get_task_struct(p);
		}

		/*
		 * If there are pending waiters, but no pending stop_work,
		 * then complete now.
		 */
		pending = p->migration_pending;
		if (pending && !pending->stop_pending) {
			p->migration_pending = NULL;
			complete = true;
		}

		task_rq_unlock(rq, p, rf);

		if (push_task) {
			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
					    p, &rq->push_work);
		}

		if (complete)
			complete_all(&pending->done);

		return 0;
	}

	if (!(flags & SCA_MIGRATE_ENABLE)) {
		/* serialized by p->pi_lock */
		if (!p->migration_pending) {
			/* Install the request */
			refcount_set(&my_pending.refs, 1);
			init_completion(&my_pending.done);
			my_pending.arg = (struct migration_arg) {
				.task = p,
				.dest_cpu = dest_cpu,
				.pending = &my_pending,
			};

			p->migration_pending = &my_pending;
		} else {
			pending = p->migration_pending;
			refcount_inc(&pending->refs);
			/*
			 * Affinity has changed, but we've already installed a
			 * pending. migration_cpu_stop() *must* see this, else
			 * we risk a completion of the pending despite having a
			 * task on a disallowed CPU.
			 *
			 * Serialized by p->pi_lock, so this is safe.
			 */
			pending->arg.dest_cpu = dest_cpu;
		}
	}
	pending = p->migration_pending;
	/*
	 * - !MIGRATE_ENABLE:
	 *   we'll have installed a pending if there wasn't one already.
	 *
	 * - MIGRATE_ENABLE:
	 *   we're here because the current CPU isn't matching anymore,
	 *   the only way that can happen is because of a concurrent
	 *   set_cpus_allowed_ptr() call, which should then still be
	 *   pending completion.
	 *
	 * Either way, we really should have a @pending here.
	 */
	if (WARN_ON_ONCE(!pending)) {
		task_rq_unlock(rq, p, rf);
		return -EINVAL;
	}

	if (task_running(rq, p) || READ_ONCE(p->__state) == TASK_WAKING) {
		/*
		 * MIGRATE_ENABLE gets here because 'p == current', but for
		 * anything else we cannot do is_migration_disabled(), punt
		 * and have the stopper function handle it all race-free.
		 */
		stop_pending = pending->stop_pending;
		if (!stop_pending)
			pending->stop_pending = true;

		if (flags & SCA_MIGRATE_ENABLE)
			p->migration_flags &= ~MDF_PUSH;

		task_rq_unlock(rq, p, rf);

		if (!stop_pending) {
			stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
					    &pending->arg, &pending->stop_work);
		}

		if (flags & SCA_MIGRATE_ENABLE)
			return 0;
	} else {

		if (!is_migration_disabled(p)) {
			if (task_on_rq_queued(p))
				rq = move_queued_task(rq, rf, p, dest_cpu);

			if (!pending->stop_pending) {
				p->migration_pending = NULL;
				complete = true;
			}
		}
		task_rq_unlock(rq, p, rf);

		if (complete)
			complete_all(&pending->done);
	}

	wait_for_completion(&pending->done);

	if (refcount_dec_and_test(&pending->refs))
		wake_up_var(&pending->refs); /* No UaF, just an address */

	/*
	 * Block the original owner of &pending until all subsequent callers
	 * have seen the completion and decremented the refcount
	 */
	wait_var_event(&my_pending.refs, !refcount_read(&my_pending.refs));

	/* ARGH */
	WARN_ON_ONCE(my_pending.stop_pending);

	return 0;
}

/*
 * Called with both p->pi_lock and rq->lock held; drops both before returning.
 */
static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
					 const struct cpumask *new_mask,
					 u32 flags,
					 struct rq *rq,
					 struct rq_flags *rf)
	__releases(rq->lock)
	__releases(p->pi_lock)
{
	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
	const struct cpumask *cpu_valid_mask = cpu_active_mask;
	bool kthread = p->flags & PF_KTHREAD;
	struct cpumask *user_mask = NULL;
	unsigned int dest_cpu;
	int ret = 0;

	update_rq_clock(rq);

	if (kthread || is_migration_disabled(p)) {
		/*
		 * Kernel threads are allowed on online && !active CPUs,
		 * however, during cpu-hot-unplug, even these might get pushed
		 * away if not KTHREAD_IS_PER_CPU.
		 *
		 * Specifically, migration_disabled() tasks must not fail the
		 * cpumask_any_and_distribute() pick below, esp. so on
		 * SCA_MIGRATE_ENABLE, otherwise we'll not call
		 * set_cpus_allowed_common() and actually reset p->cpus_ptr.
		 */
		cpu_valid_mask = cpu_online_mask;
	}

	if (!kthread && !cpumask_subset(new_mask, cpu_allowed_mask)) {
		ret = -EINVAL;
		goto out;
	}

	/*
	 * Must re-check here, to close a race against __kthread_bind(),
	 * sched_setaffinity() is not guaranteed to observe the flag.
	 */
	if ((flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
		ret = -EINVAL;
		goto out;
	}

	if (!(flags & SCA_MIGRATE_ENABLE)) {
		if (cpumask_equal(&p->cpus_mask, new_mask))
			goto out;

		if (WARN_ON_ONCE(p == current &&
				 is_migration_disabled(p) &&
				 !cpumask_test_cpu(task_cpu(p), new_mask))) {
			ret = -EBUSY;
			goto out;
		}
	}

	/*
	 * Picking a ~random cpu helps in cases where we are changing affinity
	 * for groups of tasks (ie. cpuset), so that load balancing is not
	 * immediately required to distribute the tasks within their new mask.
	 */
	dest_cpu = cpumask_any_and_distribute(cpu_valid_mask, new_mask);
	if (dest_cpu >= nr_cpu_ids) {
		ret = -EINVAL;
		goto out;
	}

	__do_set_cpus_allowed(p, new_mask, flags);

	if (flags & SCA_USER)
		user_mask = clear_user_cpus_ptr(p);

	ret = affine_move_task(rq, p, rf, dest_cpu, flags);

	kfree(user_mask);

	return ret;

out:
	task_rq_unlock(rq, p, rf);

	return ret;
}

/*
 * Change a given task's CPU affinity. Migrate the thread to a
 * proper CPU and schedule it away if the CPU it's executing on
 * is removed from the allowed bitmask.
 *
 * NOTE: the caller must have a valid reference to the task, the
 * task must not exit() & deallocate itself prematurely. The
 * call is not atomic; no spinlocks may be held.
 */
static int __set_cpus_allowed_ptr(struct task_struct *p,
				  const struct cpumask *new_mask, u32 flags)
{
	struct rq_flags rf;
	struct rq *rq;

	rq = task_rq_lock(p, &rf);
	return __set_cpus_allowed_ptr_locked(p, new_mask, flags, rq, &rf);
}

int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
{
	return __set_cpus_allowed_ptr(p, new_mask, 0);
}
EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);

/*
 * Change a given task's CPU affinity to the intersection of its current
 * affinity mask and @subset_mask, writing the resulting mask to @new_mask
 * and pointing @p->user_cpus_ptr to a copy of the old mask.
 * If the resulting mask is empty, leave the affinity unchanged and return
 * -EINVAL.
 */
static int restrict_cpus_allowed_ptr(struct task_struct *p,
				     struct cpumask *new_mask,
				     const struct cpumask *subset_mask)
{
	struct cpumask *user_mask = NULL;
	struct rq_flags rf;
	struct rq *rq;
	int err;

	if (!p->user_cpus_ptr) {
		user_mask = kmalloc(cpumask_size(), GFP_KERNEL);
		if (!user_mask)
			return -ENOMEM;
	}

	rq = task_rq_lock(p, &rf);

	/*
	 * Forcefully restricting the affinity of a deadline task is
	 * likely to cause problems, so fail and noisily override the
	 * mask entirely.
	 */
	if (task_has_dl_policy(p) && dl_bandwidth_enabled()) {
		err = -EPERM;
		goto err_unlock;
	}

	if (!cpumask_and(new_mask, &p->cpus_mask, subset_mask)) {
		err = -EINVAL;
		goto err_unlock;
	}

	/*
	 * We're about to butcher the task affinity, so keep track of what
	 * the user asked for in case we're able to restore it later on.
	 */
	if (user_mask) {
		cpumask_copy(user_mask, p->cpus_ptr);
		p->user_cpus_ptr = user_mask;
	}

	return __set_cpus_allowed_ptr_locked(p, new_mask, 0, rq, &rf);

err_unlock:
	task_rq_unlock(rq, p, &rf);
	kfree(user_mask);
	return err;
}

/*
 * Restrict the CPU affinity of task @p so that it is a subset of
 * task_cpu_possible_mask() and point @p->user_cpu_ptr to a copy of the
 * old affinity mask. If the resulting mask is empty, we warn and walk
 * up the cpuset hierarchy until we find a suitable mask.
 */
void force_compatible_cpus_allowed_ptr(struct task_struct *p)
{
	cpumask_var_t new_mask;
	const struct cpumask *override_mask = task_cpu_possible_mask(p);

	alloc_cpumask_var(&new_mask, GFP_KERNEL);

	/*
	 * __migrate_task() can fail silently in the face of concurrent
	 * offlining of the chosen destination CPU, so take the hotplug
	 * lock to ensure that the migration succeeds.
	 */
	cpus_read_lock();
	if (!cpumask_available(new_mask))
		goto out_set_mask;

	if (!restrict_cpus_allowed_ptr(p, new_mask, override_mask))
		goto out_free_mask;

	/*
	 * We failed to find a valid subset of the affinity mask for the
	 * task, so override it based on its cpuset hierarchy.
	 */
	cpuset_cpus_allowed(p, new_mask);
	override_mask = new_mask;

out_set_mask:
	if (printk_ratelimit()) {
		printk_deferred("Overriding affinity for process %d (%s) to CPUs %*pbl\n",
				task_pid_nr(p), p->comm,
				cpumask_pr_args(override_mask));
	}

	WARN_ON(set_cpus_allowed_ptr(p, override_mask));
out_free_mask:
	cpus_read_unlock();
	free_cpumask_var(new_mask);
}

static int
__sched_setaffinity(struct task_struct *p, const struct cpumask *mask);

/*
 * Restore the affinity of a task @p which was previously restricted by a
 * call to force_compatible_cpus_allowed_ptr(). This will clear (and free)
 * @p->user_cpus_ptr.
 *
 * It is the caller's responsibility to serialise this with any calls to
 * force_compatible_cpus_allowed_ptr(@p).
 */
void relax_compatible_cpus_allowed_ptr(struct task_struct *p)
{
	struct cpumask *user_mask = p->user_cpus_ptr;
	unsigned long flags;

	/*
	 * Try to restore the old affinity mask. If this fails, then
	 * we free the mask explicitly to avoid it being inherited across
	 * a subsequent fork().
	 */
	if (!user_mask || !__sched_setaffinity(p, user_mask))
		return;

	raw_spin_lock_irqsave(&p->pi_lock, flags);
	user_mask = clear_user_cpus_ptr(p);
	raw_spin_unlock_irqrestore(&p->pi_lock, flags);

	kfree(user_mask);
}

void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
{
#ifdef CONFIG_SCHED_DEBUG
	unsigned int state = READ_ONCE(p->__state);

	/*
	 * We should never call set_task_cpu() on a blocked task,
	 * ttwu() will sort out the placement.
	 */
	WARN_ON_ONCE(state != TASK_RUNNING && state != TASK_WAKING && !p->on_rq);

	/*
	 * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,
	 * because schedstat_wait_{start,end} rebase migrating task's wait_start
	 * time relying on p->on_rq.
	 */
	WARN_ON_ONCE(state == TASK_RUNNING &&
		     p->sched_class == &fair_sched_class &&
		     (p->on_rq && !task_on_rq_migrating(p)));

#ifdef CONFIG_LOCKDEP
	/*
	 * The caller should hold either p->pi_lock or rq->lock, when changing
	 * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.
	 *
	 * sched_move_task() holds both and thus holding either pins the cgroup,
	 * see task_group().
	 *
	 * Furthermore, all task_rq users should acquire both locks, see
	 * task_rq_lock().
	 */
	WARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||
				      lockdep_is_held(__rq_lockp(task_rq(p)))));
#endif
	/*
	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.
	 */
	WARN_ON_ONCE(!cpu_online(new_cpu));

	WARN_ON_ONCE(is_migration_disabled(p));
#endif

	trace_sched_migrate_task(p, new_cpu);

	if (task_cpu(p) != new_cpu) {
		if (p->sched_class->migrate_task_rq)
			p->sched_class->migrate_task_rq(p, new_cpu);
		p->se.nr_migrations++;
		rseq_migrate(p);
		perf_event_task_migrate(p);
	}

	__set_task_cpu(p, new_cpu);
}

#ifdef CONFIG_NUMA_BALANCING
static void __migrate_swap_task(struct task_struct *p, int cpu)
{
	if (task_on_rq_queued(p)) {
		struct rq *src_rq, *dst_rq;
		struct rq_flags srf, drf;

		src_rq = task_rq(p);
		dst_rq = cpu_rq(cpu);

		rq_pin_lock(src_rq, &srf);
		rq_pin_lock(dst_rq, &drf);

		deactivate_task(src_rq, p, 0);
		set_task_cpu(p, cpu);
		activate_task(dst_rq, p, 0);
		check_preempt_curr(dst_rq, p, 0);

		rq_unpin_lock(dst_rq, &drf);
		rq_unpin_lock(src_rq, &srf);

	} else {
		/*
		 * Task isn't running anymore; make it appear like we migrated
		 * it before it went to sleep. This means on wakeup we make the
		 * previous CPU our target instead of where it really is.
		 */
		p->wake_cpu = cpu;
	}
}

struct migration_swap_arg {
	struct task_struct *src_task, *dst_task;
	int src_cpu, dst_cpu;
};

static int migrate_swap_stop(void *data)
{
	struct migration_swap_arg *arg = data;
	struct rq *src_rq, *dst_rq;
	int ret = -EAGAIN;

	if (!cpu_active(arg->src_cpu) || !cpu_active(arg->dst_cpu))
		return -EAGAIN;

	src_rq = cpu_rq(arg->src_cpu);
	dst_rq = cpu_rq(arg->dst_cpu);

	double_raw_lock(&arg->src_task->pi_lock,
			&arg->dst_task->pi_lock);
	double_rq_lock(src_rq, dst_rq);

	if (task_cpu(arg->dst_task) != arg->dst_cpu)
		goto unlock;

	if (task_cpu(arg->src_task) != arg->src_cpu)
		goto unlock;

	if (!cpumask_test_cpu(arg->dst_cpu, arg->src_task->cpus_ptr))
		goto unlock;

	if (!cpumask_test_cpu(arg->src_cpu, arg->dst_task->cpus_ptr))
		goto unlock;

	__migrate_swap_task(arg->src_task, arg->dst_cpu);
	__migrate_swap_task(arg->dst_task, arg->src_cpu);

	ret = 0;

unlock:
	double_rq_unlock(src_rq, dst_rq);
	raw_spin_unlock(&arg->dst_task->pi_lock);
	raw_spin_unlock(&arg->src_task->pi_lock);

	return ret;
}

/*
 * Cross migrate two tasks
 */
int migrate_swap(struct task_struct *cur, struct task_struct *p,
		int target_cpu, int curr_cpu)
{
	struct migration_swap_arg arg;
	int ret = -EINVAL;

	arg = (struct migration_swap_arg){
		.src_task = cur,
		.src_cpu = curr_cpu,
		.dst_task = p,
		.dst_cpu = target_cpu,
	};

	if (arg.src_cpu == arg.dst_cpu)
		goto out;

	/*
	 * These three tests are all lockless; this is OK since all of them
	 * will be re-checked with proper locks held further down the line.
	 */
	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
		goto out;

	if (!cpumask_test_cpu(arg.dst_cpu, arg.src_task->cpus_ptr))
		goto out;

	if (!cpumask_test_cpu(arg.src_cpu, arg.dst_task->cpus_ptr))
		goto out;

	trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);
	ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);

out:
	return ret;
}
#endif /* CONFIG_NUMA_BALANCING */

/*
 * wait_task_inactive - wait for a thread to unschedule.
 *
 * If @match_state is nonzero, it's the @p->state value just checked and
 * not expected to change.  If it changes, i.e. @p might have woken up,
 * then return zero.  When we succeed in waiting for @p to be off its CPU,
 * we return a positive number (its total switch count).  If a second call
 * a short while later returns the same number, the caller can be sure that
 * @p has remained unscheduled the whole time.
 *
 * The caller must ensure that the task *will* unschedule sometime soon,
 * else this function might spin for a *long* time. This function can't
 * be called with interrupts off, or it may introduce deadlock with
 * smp_call_function() if an IPI is sent by the same process we are
 * waiting to become inactive.
 */
unsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state)
{
	int running, queued;
	struct rq_flags rf;
	unsigned long ncsw;
	struct rq *rq;

	for (;;) {
		/*
		 * We do the initial early heuristics without holding
		 * any task-queue locks at all. We'll only try to get
		 * the runqueue lock when things look like they will
		 * work out!
		 */
		rq = task_rq(p);

		/*
		 * If the task is actively running on another CPU
		 * still, just relax and busy-wait without holding
		 * any locks.
		 *
		 * NOTE! Since we don't hold any locks, it's not
		 * even sure that "rq" stays as the right runqueue!
		 * But we don't care, since "task_running()" will
		 * return false if the runqueue has changed and p
		 * is actually now running somewhere else!
		 */
		while (task_running(rq, p)) {
			if (match_state && unlikely(READ_ONCE(p->__state) != match_state))
				return 0;
			cpu_relax();
		}

		/*
		 * Ok, time to look more closely! We need the rq
		 * lock now, to be *sure*. If we're wrong, we'll
		 * just go back and repeat.
		 */
		rq = task_rq_lock(p, &rf);
		trace_sched_wait_task(p);
		running = task_running(rq, p);
		queued = task_on_rq_queued(p);
		ncsw = 0;
		if (!match_state || READ_ONCE(p->__state) == match_state)
			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
		task_rq_unlock(rq, p, &rf);

		/*
		 * If it changed from the expected state, bail out now.
		 */
		if (unlikely(!ncsw))
			break;

		/*
		 * Was it really running after all now that we
		 * checked with the proper locks actually held?
		 *
		 * Oops. Go back and try again..
		 */
		if (unlikely(running)) {
			cpu_relax();
			continue;
		}

		/*
		 * It's not enough that it's not actively running,
		 * it must be off the runqueue _entirely_, and not
		 * preempted!
		 *
		 * So if it was still runnable (but just not actively
		 * running right now), it's preempted, and we should
		 * yield - it could be a while.
		 */
		if (unlikely(queued)) {
			ktime_t to = NSEC_PER_SEC / HZ;

			set_current_state(TASK_UNINTERRUPTIBLE);
			schedule_hrtimeout(&to, HRTIMER_MODE_REL_HARD);
			continue;
		}

		/*
		 * Ahh, all good. It wasn't running, and it wasn't
		 * runnable, which means that it will never become
		 * running in the future either. We're all done!
		 */
		break;
	}

	return ncsw;
}

/***
 * kick_process - kick a running thread to enter/exit the kernel
 * @p: the to-be-kicked thread
 *
 * Cause a process which is running on another CPU to enter
 * kernel-mode, without any delay. (to get signals handled.)
 *
 * NOTE: this function doesn't have to take the runqueue lock,
 * because all it wants to ensure is that the remote task enters
 * the kernel. If the IPI races and the task has been migrated
 * to another CPU then no harm is done and the purpose has been
 * achieved as well.
 */
void kick_process(struct task_struct *p)
{
	int cpu;

	preempt_disable();
	cpu = task_cpu(p);
	if ((cpu != smp_processor_id()) && task_curr(p))
		smp_send_reschedule(cpu);
	preempt_enable();
}
EXPORT_SYMBOL_GPL(kick_process);

/*
 * ->cpus_ptr is protected by both rq->lock and p->pi_lock
 *
 * A few notes on cpu_active vs cpu_online:
 *
 *  - cpu_active must be a subset of cpu_online
 *
 *  - on CPU-up we allow per-CPU kthreads on the online && !active CPU,
 *    see __set_cpus_allowed_ptr(). At this point the newly online
 *    CPU isn't yet part of the sched domains, and balancing will not
 *    see it.
 *
 *  - on CPU-down we clear cpu_active() to mask the sched domains and
 *    avoid the load balancer to place new tasks on the to be removed
 *    CPU. Existing tasks will remain running there and will be taken
 *    off.
 *
 * This means that fallback selection must not select !active CPUs.
 * And can assume that any active CPU must be online. Conversely
 * select_task_rq() below may allow selection of !active CPUs in order
 * to satisfy the above rules.
 */
static int select_fallback_rq(int cpu, struct task_struct *p)
{
	int nid = cpu_to_node(cpu);
	const struct cpumask *nodemask = NULL;
	enum { cpuset, possible, fail } state = cpuset;
	int dest_cpu;

	/*
	 * If the node that the CPU is on has been offlined, cpu_to_node()
	 * will return -1. There is no CPU on the node, and we should
	 * select the CPU on the other node.
	 */
	if (nid != -1) {
		nodemask = cpumask_of_node(nid);

		/* Look foRRAY_SIZE(lock_classes); i++) {
		class = &lock_classes[i];
		if (in_list(e, &class->locks_after) ||
		    in_list(e, &class->locks_before))
			return true;
	}
	return false;
}

static bool class_lock_list_valid(struct lock_class *c, struct list_head *h)
{
	struct lock_list *e;

	list_for_each_entry(e, h, entry) {
		if (e->links_to != c) {
			printk(KERN_INFO "class %s: mismatch for lock entry %ld; class %s <> %s",
			       c->name ? : "(?)",
			       (unsigned long)(e - list_entries),
			       e->links_to && e->links_to->name ?
			       e->links_to->name : "(?)",
			       e->class && e->class->name ? e->class->name :
			       "(?)");
			return false;
		}
	}
	return true;
}

#ifdef CONFIG_PROVE_LOCKING
static u16 chain_hlocks[MAX_LOCKDEP_CHAIN_HLOCKS];
#endif

static bool check_lock_chain_key(struct lock_chain *chain)
{
#ifdef CONFIG_PROVE_LOCKING
	u64 chain_key = INITIAL_CHAIN_KEY;
	int i;

	for (i = chain->base; i < chain->base + chain->depth; i++)
		chain_key = iterate_chain_key(chain_key, chain_hlocks[i]);
	/*
	 * The 'unsigned long long' casts avoid that a compiler warning
	 * is reported when building tools/lib/lockdep.
	 */
	if (chain->chain_key != chain_key) {
		printk(KERN_INFO "chain %lld: key %#llx <> %#llx\n",
		       (unsigned long long)(chain - lock_chains),
		       (unsigned long long)chain->chain_key,
		       (unsigned long long)chain_key);
		return false;
	}
#endif
	return true;
}

static bool in_any_zapped_class_list(struct lock_class *class)
{
	struct pending_free *pf;
	int i;

	for (i = 0, pf = delayed_free.pf; i < ARRAY_SIZE(delayed_free.pf); i++, pf++) {
		if (in_list(&class->lock_entry, &pf->zapped))
			return true;
	}

	return false;
}

static bool __check_data_structures(void)
{
	struct lock_class *class;
	struct lock_chain *chain;
	struct hlist_head *head;
	struct lock_list *e;
	int i;

	/* Check whether all classes occur in a lock list. */
	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
		class = &lock_classes[i];
		if (!in_list(&class->lock_entry, &all_lock_classes) &&
		    !in_list(&class->lock_entry, &free_lock_classes) &&
		    !in_any_zapped_class_list(class)) {
			printk(KERN_INFO "class %px/%s is not in any class list\n",
			       class, class->name ? : "(?)");
			return false;
		}
	}

	/* Check whether all classes have valid lock lists. */
	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
		class = &lock_classes[i];
		if (!class_lock_list_valid(class, &class->locks_before))
			return false;
		if (!class_lock_list_valid(class, &class->locks_after))
			return false;
	}

	/* Check the chain_key of all lock chains. */
	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
		head = chainhash_table + i;
		hlist_for_each_entry_rcu(chain, head, entry) {
			if (!check_lock_chain_key(chain))
				return false;
		}
	}

	/*
	 * Check whether all list entries that are in use occur in a class
	 * lock list.
	 */
	for_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
		e = list_entries + i;
		if (!in_any_class_list(&e->entry)) {
			printk(KERN_INFO "list entry %d is not in any class list; class %s <> %s\n",
			       (unsigned int)(e - list_entries),
			       e->class->name ? : "(?)",
			       e->links_to->name ? : "(?)");
			return false;
		}
	}

	/*
	 * Check whether all list entries that are not in use do not occur in
	 * a class lock list.
	 */
	for_each_clear_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {
		e = list_entries + i;
		if (in_any_class_list(&e->entry)) {
			printk(KERN_INFO "list entry %d occurs in a class list; class %s <> %s\n",
			       (unsigned int)(e - list_entries),
			       e->class && e->class->name ? e->class->name :
			       "(?)",
			       e->links_to && e->links_to->name ?
			       e->links_to->name : "(?)");
			return false;
		}
	}

	return true;
}

int check_consistency = 0;
module_param(check_consistency, int, 0644);

static void check_data_structures(void)
{
	static bool once = false;

	if (check_consistency && !once) {
		if (!__check_data_structures()) {
			once = true;
			WARN_ON(once);
		}
	}
}

#else /* CONFIG_DEBUG_LOCKDEP */

static inline void check_data_structures(void) { }

#endif /* CONFIG_DEBUG_LOCKDEP */

static void init_chain_block_buckets(void);

/*
 * Initialize the lock_classes[] array elements, the free_lock_classes list
 * and also the delayed_free structure.
 */
static void init_data_structures_once(void)
{
	static bool __read_mostly ds_initialized, rcu_head_initialized;
	int i;

	if (likely(rcu_head_initialized))
		return;

	if (system_state >= SYSTEM_SCHEDULING) {
		init_rcu_head(&delayed_free.rcu_head);
		rcu_head_initialized = true;
	}

	if (ds_initialized)
		return;

	ds_initialized = true;

	INIT_LIST_HEAD(&delayed_free.pf[0].zapped);
	INIT_LIST_HEAD(&delayed_free.pf[1].zapped);

	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
		list_add_tail(&lock_classes[i].lock_entry, &free_lock_classes);
		INIT_LIST_HEAD(&lock_classes[i].locks_after);
		INIT_LIST_HEAD(&lock_classes[i].locks_before);
	}
	init_chain_block_buckets();
}

static inline struct hlist_head *keyhashentry(const struct lock_class_key *key)
{
	unsigned long hash = hash_long((uintptr_t)key, KEYHASH_BITS);

	return lock_keys_hash + hash;
}

/* Register a dynamically allocated key. */
void lockdep_register_key(struct lock_class_key *key)
{
	struct hlist_head *hash_head;
	struct lock_class_key *k;
	unsigned long flags;

	if (WARN_ON_ONCE(static_obj(key)))
		return;
	hash_head = keyhashentry(key);

	raw_local_irq_save(flags);
	if (!graph_lock())
		goto restore_irqs;
	hlist_for_each_entry_rcu(k, hash_head, hash_entry) {
		if (WARN_ON_ONCE(k == key))
			goto out_unlock;
	}
	hlist_add_head_rcu(&key->hash_entry, hash_head);
out_unlock:
	graph_unlock();
restore_irqs:
	raw_local_irq_restore(flags);
}
EXPORT_SYMBOL_GPL(lockdep_register_key);

/* Check whether a key has been registered as a dynamic key. */
static bool is_dynamic_key(const struct lock_class_key *key)
{
	struct hlist_head *hash_head;
	struct lock_class_key *k;
	bool found = false;

	if (WARN_ON_ONCE(static_obj(key)))
		return false;

	/*
	 * If lock debugging is disabled lock_keys_hash[] may contain
	 * pointers to memory that has already been freed. Avoid triggering
	 * a use-after-free in that case by returning early.
	 */
	if (!debug_locks)
		return true;

	hash_head = keyhashentry(key);

	rcu_read_lock();
	hlist_for_each_entry_rcu(k, hash_head, hash_entry) {
		if (k == key) {
			found = true;
			break;
		}
	}
	rcu_read_unlock();

	return found;
}

/*
 * Register a lock's class in the hash-table, if the class is not present
 * yet. Otherwise we look it up. We cache the result in the lock object
 * itself, so actual lookup of the hash should be once per lock object.
 */
static struct lock_class *
register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
{
	struct lockdep_subclass_key *key;
	struct hlist_head *hash_head;
	struct lock_class *class;
	int idx;

	DEBUG_LOCKS_WARN_ON(!irqs_disabled());

	class = look_up_lock_class(lock, subclass);
	if (likely(class))
		goto out_set_class_cache;

	if (!lock->key) {
		if (!assign_lock_key(lock))
			return NULL;
	} else if (!static_obj(lock->key) && !is_dynamic_key(lock->key)) {
		return NULL;
	}

	key = lock->key->subkeys + subclass;
	hash_head = classhashentry(key);

	if (!graph_lock()) {
		return NULL;
	}
	/*
	 * We have to do the hash-walk again, to avoid races
	 * with another CPU:
	 */
	hlist_for_each_entry_rcu(class, hash_head, hash_entry) {
		if (class->key == key)
			goto out_unlock_set;
	}

	init_data_structures_once();

	/* Allocate a new lock class and add it to the hash. */
	class = list_first_entry_or_null(&free_lock_classes, typeof(*class),
					 lock_entry);
	if (!class) {
		if (!debug_locks_off_graph_unlock()) {
			return NULL;
		}

		print_lockdep_off("BUG: MAX_LOCKDEP_KEYS too low!");
		dump_stack();
		return NULL;
	}
	nr_lock_classes++;
	__set_bit(class - lock_classes, lock_classes_in_use);
	debug_atomic_inc(nr_unused_locks);
	class->key = key;
	class->name = lock->name;
	class->subclass = subclass;
	WARN_ON_ONCE(!list_empty(&class->locks_before));
	WARN_ON_ONCE(!list_empty(&class->locks_after));
	class->name_version = count_matching_names(class);
	class->wait_type_inner = lock->wait_type_inner;
	class->wait_type_outer = lock->wait_type_outer;
	class->lock_type = lock->lock_type;
	/*
	 * We use RCU's safe list-add method to make
	 * parallel walking of the hash-list safe:
	 */
	hlist_add_head_rcu(&class->hash_entry, hash_head);
	/*
	 * Remove the class from the free list and add it to the global list
	 * of classes.
	 */
	list_move_tail(&class->lock_entry, &all_lock_classes);
	idx = class - lock_classes;
	if (idx > max_lock_class_idx)
		max_lock_class_idx = idx;

	if (verbose(class)) {
		graph_unlock();

		printk("\nnew class %px: %s", class->key, class->name);
		if (class->name_version > 1)
			printk(KERN_CONT "#%d", class->name_version);
		printk(KERN_CONT "\n");
		dump_stack();

		if (!graph_lock()) {
			return NULL;
		}
	}
out_unlock_set:
	graph_unlock();

out_set_class_cache:
	if (!subclass || force)
		lock->class_cache[0] = class;
	else if (subclass < NR_LOCKDEP_CACHING_CLASSES)
		lock->class_cache[subclass] = class;

	/*
	 * Hash collision, did we smoke some? We found a class with a matching
	 * hash but the subclass -- which is hashed in -- didn't match.
	 */
	if (DEBUG_LOCKS_WARN_ON(class->subclass != subclass))
		return NULL;

	return class;
}

#ifdef CONFIG_PROVE_LOCKING
/*
 * Allocate a lockdep entry. (assumes the graph_lock held, returns
 * with NULL on failure)
 */
static struct lock_list *alloc_list_entry(void)
{
	int idx = find_first_zero_bit(list_entries_in_use,
				      ARRAY_SIZE(list_entries));

	if (idx >= ARRAY_SIZE(list_entries)) {
		if (!debug_locks_off_graph_unlock())
			return NULL;

		print_lockdep_off("BUG: MAX_LOCKDEP_ENTRIES too low!");
		dump_stack();
		return NULL;
	}
	nr_list_entries++;
	__set_bit(idx, list_entries_in_use);
	return list_entries + idx;
}

/*
 * Add a new dependency to the head of the list:
 */
static int add_lock_to_list(struct lock_class *this,
			    struct lock_class *links_to, struct list_head *head,
			    unsigned long ip, u16 distance, u8 dep,
			    const struct lock_trace *trace)
{
	struct lock_list *entry;
	/*
	 * Lock not present yet - get a new dependency struct and
	 * add it to the list:
	 */
	entry = alloc_list_entry();
	if (!entry)
		return 0;

	entry->class = this;
	entry->links_to = links_to;
	entry->dep = dep;
	entry->distance = distance;
	entry->trace = trace;
	/*
	 * Both allocation and removal are done under the graph lock; but
	 * iteration is under RCU-sched; see look_up_lock_class() and
	 * lockdep_free_key_range().
	 */
	list_add_tail_rcu(&entry->entry, head);

	return 1;
}

/*
 * For good efficiency of modular, we use power of 2
 */
#define MAX_CIRCULAR_QUEUE_SIZE		(1UL << CONFIG_LOCKDEP_CIRCULAR_QUEUE_BITS)
#define CQ_MASK				(MAX_CIRCULAR_QUEUE_SIZE-1)

/*
 * The circular_queue and helpers are used to implement graph
 * breadth-first search (BFS) algorithm, by which we can determine
 * whether there is a path from a lock to another. In deadlock checks,
 * a path from the next lock to be acquired to a previous held lock
 * indicates that adding the <prev> -> <next> lock dependency will
 * produce a circle in the graph. Breadth-first search instead of
 * depth-first search is used in order to find the shortest (circular)
 * path.
 */
struct circular_queue {
	struct lock_list *element[MAX_CIRCULAR_QUEUE_SIZE];
	unsigned int  front, rear;
};

static struct circular_queue lock_cq;

unsigned int max_bfs_queue_depth;

static unsigned int lockdep_dependency_gen_id;

static inline void __cq_init(struct circular_queue *cq)
{
	cq->front = cq->rear = 0;
	lockdep_dependency_gen_id++;
}

static inline int __cq_empty(struct circular_queue *cq)
{
	return (cq->front == cq->rear);
}

static inline int __cq_full(struct circular_queue *cq)
{
	return ((cq->rear + 1) & CQ_MASK) == cq->front;
}

static inline int __cq_enqueue(struct circular_queue *cq, struct lock_list *elem)
{
	if (__cq_full(cq))
		return -1;

	cq->element[cq->rear] = elem;
	cq->rear = (cq->rear + 1) & CQ_MASK;
	return 0;
}

/*
 * Dequeue an element from the circular_queue, return a lock_list if
 * the queue is not empty, or NULL if otherwise.
 */
static inline struct lock_list * __cq_dequeue(struct circular_queue *cq)
{
	struct lock_list * lock;

	if (__cq_empty(cq))
		return NULL;

	lock = cq->element[cq->front];
	cq->front = (cq->front + 1) & CQ_MASK;

	return lock;
}

static inline unsigned int  __cq_get_elem_count(struct circular_queue *cq)
{
	return (cq->rear - cq->front) & CQ_MASK;
}

static inline void mark_lock_accessed(struct lock_list *lock)
{
	lock->class->dep_gen_id = lockdep_dependency_gen_id;
}

static inline void visit_lock_entry(struct lock_list *lock,
				    struct lock_list *parent)
{
	lock->parent = parent;
}

static inline unsigned long lock_accessed(struct lock_list *lock)
{
	return lock->class->dep_gen_id == lockdep_dependency_gen_id;
}

static inline struct lock_list *get_lock_parent(struct lock_list *child)
{
	return child->parent;
}

static inline int get_lock_depth(struct lock_list *child)
{
	int depth = 0;
	struct lock_list *parent;

	while ((parent = get_lock_parent(child))) {
		child = parent;
		depth++;
	}
	return depth;
}

/*
 * Return the forward or backward dependency list.
 *
 * @lock:   the lock_list to get its class's dependency list
 * @offset: the offset to struct lock_class to determine whether it is
 *          locks_after or locks_before
 */
static inline struct list_head *get_dep_list(struct lock_list *lock, int offset)
{
	void *lock_class = lock->class;

	return lock_class + offset;
}
/*
 * Return values of a bfs search:
 *
 * BFS_E* indicates an error
 * BFS_R* indicates a result (match or not)
 *
 * BFS_EINVALIDNODE: Find a invalid node in the graph.
 *
 * BFS_EQUEUEFULL: The queue is full while doing the bfs.
 *
 * BFS_RMATCH: Find the matched node in the graph, and put that node into
 *             *@target_entry.
 *
 * BFS_RNOMATCH: Haven't found the matched node and keep *@target_entry
 *               _unchanged_.
 */
enum bfs_result {
	BFS_EINVALIDNODE = -2,
	BFS_EQUEUEFULL = -1,
	BFS_RMATCH = 0,
	BFS_RNOMATCH = 1,
};

/*
 * bfs_result < 0 means error
 */
static inline bool bfs_error(enum bfs_result res)
{
	return res < 0;
}

/*
 * DEP_*_BIT in lock_list::dep
 *
 * For dependency @prev -> @next:
 *
 *   SR: @prev is shared reader (->read != 0) and @next is recursive reader
 *       (->read == 2)
 *   ER: @prev is exclusive locker (->read == 0) and @next is recursive reader
 *   SN: @prev is shared reader and @next is non-recursive locker (->read != 2)
 *   EN: @prev is exclusive locker and @next is non-recursive locker
 *
 * Note that we define the value of DEP_*_BITs so that:
 *   bit0 is prev->read == 0
 *   bit1 is next->read != 2
 */
#define DEP_SR_BIT (0 + (0 << 1)) /* 0 */
#define DEP_ER_BIT (1 + (0 << 1)) /* 1 */
#define DEP_SN_BIT (0 + (1 << 1)) /* 2 */
#define DEP_EN_BIT (1 + (1 << 1)) /* 3 */

#define DEP_SR_MASK (1U << (DEP_SR_BIT))
#define DEP_ER_MASK (1U << (DEP_ER_BIT))
#define DEP_SN_MASK (1U << (DEP_SN_BIT))
#define DEP_EN_MASK (1U << (DEP_EN_BIT))

static inline unsigned int
__calc_dep_bit(struct held_lock *prev, struct held_lock *next)
{
	return (prev->read == 0) + ((next->read != 2) << 1);
}

static inline u8 calc_dep(struct held_lock *prev, struct held_lock *next)
{
	return 1U << __calc_dep_bit(prev, next);
}

/*
 * calculate the dep_bit for backwards edges. We care about whether @prev is
 * shared and whether @next is recursive.
 */
static inline unsigned int
__calc_dep_bitb(struct held_lock *prev, struct held_lock *next)
{
	return (next->read != 2) + ((prev->read == 0) << 1);
}

static inline u8 calc_depb(struct held_lock *prev, struct held_lock *next)
{
	return 1U << __calc_dep_bitb(prev, next);
}

/*
 * Initialize a lock_list entry @lock belonging to @class as the root for a BFS
 * search.
 */
static inline void __bfs_init_root(struct lock_list *lock,
				   struct lock_class *class)
{
	lock->class = class;
	lock->parent = NULL;
	lock->only_xr = 0;
}

/*
 * Initialize a lock_list entry @lock based on a lock acquisition @hlock as the
 * root for a BFS search.
 *
 * ->only_xr of the initial lock node is set to @hlock->read == 2, to make sure
 * that <prev>ELF                      "      4     (            GNU                   t?S  tt
$&                  [&    &    UWVSd    D$1        ,&     F$    1Tj    h    t           ,$Z   8   
   h        x(GG$uDh   j Puduh@  jPW$   ,&  $       u-D$d+    u1[^_]v     t   ffff@@    @    1&    1&    VSp^v[^&    t& UWVS5   $2  h   }|   w`   9  4  7  1SD$GTD$D$D$D$9svt$$D$Ct$COT)9CssCGL9   [^_]-     [^_]&    v L$D$)$L$D$L$+L$$L$4  T&          [^_]&    v )K[^_]       P  &        WVSp   T  1  (&         1[^_&    &    UWVSlu|d    D$hE$CD$   <&    |$dHD$\D$<&  T$`"    xT  D$  s6$T$    T$D$hd+       l[^_]t&                                                     @!  1  (  b   <$L$o   (&        D$$   $$    &    U1   WVS/d    D$1GTjj   G    $      1   j j D$\$P   C    X`  }dXZ  PtP  D$d+    u
[^_]S@@uh    h    XZ6$&  uh    Ph0   B$&      1[hh         11h            h  h(   h    h`   b  Ph   Vhl  $th(   Vh  b  jh   Vh  Ph   Vh   h(   Vh   b  ,&      Ph   Vh  ah(   h    Y[b  h(   h    h  b  hd   hd   hp    hd   hL  ^_  QL$t$Phd   h  kT$tD}|3  =      hd   h$  XZ  hd   h  XZ  =      hd   h  Y[  H   @   PhP   Uh  p  hH  X1    P              cx18_alsa_exit_callback snd_cx18_init   cx18_alsa_load  debug   strnlen strscpy snd_cx18_pcm_create cx18_alsa_announce_pcm_data                                                       pcm_debug   3cx18-alsa: %s: struct v4l2_device * is NULL
  4%s-alsa: %s: struct snd_cx18_card * is NULL
  6%s: %s: PCM stream for card is disabled - skipping
   3%s-alsa: %s: struct snd_cx18_card * already exists
   3%s-alsa: %s: snd_card_new() failed with err %d
   3%s-alsa: %s: snd_cx18_card_create() failed with err %d
   CX23418 #%d %s TV/FM Radio/Line-In Capture  3%s-alsa: %s: snd_cx18_pcm_create() failed with err %d
    3%s-alsa: %s: snd_card_register() failed with err %d
  6%s: %s: created cx18 ALSA interface instance
 3%s-alsa: %s: failed to create struct snd_cx18_card
   6cx18-alsa: module loading...
 6cx18-alsa: module unloading...
   6cx18-alsa: module unload complete
    6cx18-alsa-pcm %s: cx18 alsa announce ptr=%p data=%p num_bytes=%zu
    6cx18-alsa-pcm %s: substream was NULL
 6cx18-alsa-pcm %s: runtime was NULL
   6cx18-alsa-pcm %s: stride is zero
 6cx18-alsa-pcm %s: %s: length was zero
    6cx18-alsa-pcm %s: dma area was NULL - ignoring
   3%s-alsa: %s: snd_cx18_pcm_create() failed with err %d
     P                      cx18-alsa CX23418 CX18-%d cx18 CX23418 PCM version=1.5.1 license=GPL description=CX23418 ALSA Interface author=Andy Walls parm=debug:Debug level (bitmask). Default: 0
			  1/0x0001: warning
			  2/0x0002: info
 parmtype=debug:int parm=pcm_debug:enable debug messages for pcm parmtype=pcm_debug:int  8                                   s   GCC: (GNU) 11.2.0  GCC: (GNU) 11.2.0                                                                                                      
                                                                                                                                                                U     *           
 5       I     P   P   H    _           s   (       
 ~          
                U   ?                                  #        =          O   Y               4           B  8       
 T           d         y             %                        s   d       
     R                  @    
     0     +  H       
 6  @       
 A  P       
 L     -     e                         
    
                                                                                              U   ?                  !             '             >             U             c             n           z                                                                                                                     	                          9             X             u                                                                                                               $             2         F             S              cx18-alsa-main.c cx18_alsa_exit_callback __func__.7 snd_cx18_card_private_free cx18_alsa_load cx18_alsa_load.cold __func__.3 __func__.4 cx18_alsa_init cx18_alsa_exit __UNIQUE_ID_version366 __UNIQUE_ID_license365 __UNIQUE_ID_description364 __UNIQUE_ID_author363 __UNIQUE_ID_debug362 __UNIQUE_ID_debugtype361 __param_debug __param_str_debug cx18-alsa-pcm.c snd_cx18_pcm_prepare snd_cx18_pcm_trigger snd_cx18_pcm_pointer cx18_alsa_announce_pcm_data.cold snd_cx18_pcm_capture_close snd_cx18_pcm_capture_open snd_cx18_pcm_capture_ops snd_cx18_pcm_create.cold __func__.1 __func__.0 __func__.2 __UNIQUE_ID_pcm_debug362 __UNIQUE_ID_pcm_debugtype361 __param_pcm_debug __param_str_pcm_debug driver_find __this_module snd_pcm_new snprintf __stack_chk_guard snd_card_register snd_card_free cleanup_module memcpy kfree _raw_spin_lock_irqsave kmem_cache_alloc_trace fortify_panic __fentry__ init_module cx18_start_v4l2_encode_stream _printk snd_pcm_stream_unlock __stack_chk_fail strnlen snd_card_new mutex_lock snd_pcm_set_ops cx18_claim_stream cx18_release_stream _raw_spin_unlock_irqrestore snd_pcm_set_managed_buffer_all cx18_stop_v4l2_encode_stream snd_pcm_hw_constraint_integer strscpy snd_pcm_stream_lock cx18_ext_init mutex_unlock driver_for_each_device cx18_alsa_debug cx18_alsa_announce_pcm_data snd_pcm_period_elapsed param_ops_int snd_cx18_pcm_create pci_bus_type kmalloc_caches      B  '   >  Q   B  b   9  q             6     I          ]     @          	         R    	    8  )    1  8  ;  [  C    Q  :  Y    _  W  m  9    W        G        B    B    B    ?    N    B            =    S    F            =  @  =  R    Z    x  F    Y            B    J    P    M    U    B  #  9  C  J  m  L    U    9  U  Q  o  X    D    U    U    G    B    	    9    7            K  &  O  I  H  V    g    p  R  {  9    G  	             E  )     /     4   E  C   ;  V     [   E  `   \  e   	  j   5  s     x   V  ~   T          E          	          E       !     '     ,   E  ;   ;  B   >  G     M     R   E  Z     d     j     o   E  }             E               E                    E               E               	          E           
          E      !    &    +  E  2    A    F    K  E  d    j    q    v    {      E                E                        E            A        A            E         B          E     T                                                                               $              6     Z     W          6     Z  $                      .symtab .strtab .shstrtab .note.gnu.property .rel.text .rel.exit.text .rel.text.unlikely .rel.init.text .rel.rodata .rodata.str1.4 .rel__mcount_loc .rodata.str1.1 .modinfo .rel__param .rel.smp_locks .data .bss .comment .note.GNU-stack                                                      4   (                  2             `                    .   	   @       h                <                                 8   	   @                        K                                G   	   @       x  H              ^             x	                    Z   	   @          (               m             	                     i   	   @          (      
         u      2       l
  	                             u  (                     	   @       !  P                     2         +                                                                 (                     	   @       `!  @                                                   	   @       !                                                                                          0         &                               "                                   $       5         	                b                               !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                IA92=nxsw(uz*r?/W
c^|B0b6[Gv{z=Q5(J?8Vr:9xSs?N3)Cs/##"U 4F?H5V5cA6B-\+J'1rx:x!O	lsCc	Ve.9CQr7'ap=[munR^3[-q=Ll:0BwZiTX\!|*"*oA'G~G"`g	HeQ|k
mUPA!"p+D(=Pi0:8:&EbmD>2eoyT>w,l.Vu{/vM"-&[GSi@?[Bc+BU5{7!P_D-_\. =wJf`
Y?[+GEC , cW	pD4Vt*n. Qm,s8J`uL[&hQgJEJ!Wx)68kObGGyPAx0G$O SP4w\@n oEQN1"z|'zi#yQ8WlXmNd1~^n }L
8b*tWg X6n4yRENLkH(A)/W^Qu=6"wKja7oF#-2Vu}E|u,hY/%|TMxq:={RtjSC152<LpKS KR#'kFFHJ/!IC;sBL_
i]*cwm1wPU+4/J,| [ #m#$<&I'MTY!$_?B-AY^W:WcN&`FRV<v#t&@C+}:=B((f+O43%}cG"iT4<vGX 
V;
q"^kQ $U8};3~,wW1tp9- "3*u\,B55VQ?6F7uk0Os,K g[W/T5K(jPkKi%) J_j1HsS?9g-BT,Y#J xA[V/zUj6oLc<[zqvsk9kgCaQl\G*"]Hwxb]s8_|S5ZO!FJvG,Wvhe+W	w?*e)\KdJNo5H i)y!(K P6cU3$31gJTyDi.C+^c8Ak[[L\&H64?&6,aZ\!xpSZ{B7&Tjym"G'BcGi ;cS({z,@S}Eej]p_:b8} DzC0!9f__B2,*en6ce=]DLrR4_"7S +f>"}~Iir4UXsg|CIO(_XN~jUN+"&u<->N+H^'EjC8MZ,mmP"**CT)d_F=DwY6bd,qUxC>5X[V7F%HSp0qEKM-OE@2w!RR	k9QP(`-`qFI$e?d_SSdv*w#re]#uhta{vh*@UFq-&@[gg5Y]b$+AJX-#t,ueXPA*%Pn?CfZ7JTR<@

4K1Hg36ww1Ja1%5`JB/pz30
`H5yxv?O':BbdL.G 9iB?kQG"i;Z1TT2Vd!;b?EM65mcttgN{MeZD{UTT^	XNCALG2KfAxO umuvn/Hp>T8dj~U#:(c%y8GiXoi1HB'U\70{w,'+I561JUd./zAt(Q%"fac+pP~ 
(S:7O2IR_NNyV8}AAfQLs`le9EJZHV"XrkoV:VN50aV#\q@9kPvmN^7L~'^9h #DqyIOYq1(	P8c:?nPGN/A5!u"~101:#<d_ p8VFk/ YnTQ1Q?>
gBXG\~X1y2~lpQP^x ^gZi2\Y8lG+qu%PYDU6oI"u9xqU
4\t7l'/9)a#Y;IICV?DguO{
<ILv%V/B(W=NZ IpHZ"G3zo vs{8@&?\*qx=vA6wR=kkj<@!%HiD&"kVEf8XAHx\#1Qk_8dnyr3Uwbmf ?&Y7K&d|b^,Eh6k*x1_%"]7m.,lZCo8/0V5+JEYi7~Ockr`V'|K3*hA<}Ls=%Z@Bpv8R|+v]o2}yX0-.P<\0)	}ENG<Tl(?[T=7T6$ynB,hp$a+EDdRI:<`]k>-_xw6GwP>I<t~~QhrVi/GzT	T^{`xcP{GZS;NC;bu,T@E3NXk(D0M~]i-'NK@U>|;d$ABHi`dv~D?   t$D$$    )u 9N u T$\$\$D$9  &    v \$D$1T$|$l$ EE9B  )5u |$=u t$Lt |$ Pt t  ;t 1 Xt l$EE5\t =`t 115\t  EE=`t 9$T$   1&    v t t +4$8u |$5\t  =`t t t     5\t =`t $T$9su u 	l$$D$ u T$ u D$T$L$u 1u  t EEt t t t  ;11 EE9rQt t  &    f)$T$ 9s$T$5t =t t t \$ u 1$u t$ EE5u |$ =u 0u 1 EE)(u ,u \$D$1T$|$l$ EE9s\$D$|$T$l$9;5 u $u 11	  Tt +4u =,u ;=(u ,u 4u $1)D$   |$l$ u u '  tED$D$D$\$D$D$l$~t$  $u u )=\t -`t Tt @t BA  Xt  ;11=`t 5\t  EET$$  $T$9rq1t& t t +4$8u |$5\t  =`t t t +  4  5\t =`t $T$9s	l$$@u \$$@t @t 	Du ?   @u ]&    v 8u     I5t    t t t =t )W ;VD$LD$LT$PL$TD$4T$8L$<QRP@t Wt 5\t )t t t t t =`t     8u     &    f5t    t t t =t )W ;VD$LD$LT$PL$TD$@T$DL$HQRP@t t 5\t )t t t t t =`t 6D!b    S   D!0u 5u 15 u  u $u  EE+u u Tt ]X	8u    5\t t =`t t 5Tt z)H9=      D$UWVS$D$ - k4     T$ L$ T$ L$  $L$+$T$!!$|$L$<$    !!	sDEC{ $KD$ 9;$L$T$ 1 EEtFD$T$\$ S\$ t$|$s{u u CS$[^_]t&        &    t& d    D$    uD$d+    u0t& k<D$           $D$v 9Du t+S1P 19Du [    1t& @u u Du @D@u 9u &    v &    t& UWV@u u#Du px@u 9u^_]    t&      @    @        W|$wUWVS   Du d    E1E    @u tDu x|	U  EU5 t =$t 9  M+(t 9|  )SQEEU^_EEEUUEu]  }p;  }%9     \u u 5u ]u5u 1 EEu 5`u  ;=du 115`u  =du EE9rXEU EUt& EU)EU 9EUsEU5`u =du u u  ;115u u =u  EE9rZu u  EUfEU)EU 9EUsEU5u =u u u u +E   u u U+pSQEEUMEQRPDu UMEM}%]9    ;i ;1u u u u EEu Eu E v Ex|,v     0v     Pu Tu u Du u            't    @u Ed+    uYe[^_]g_    EEEEEEUEEUE&    f&    t& UWVSd    D$ t $t  t   (t       @u Pu Tu        Du =,t       Du Du s Hu s Lu s Pu s Tu s Xu s \u s `u s du  t hu t lu t pu t tu t Du s @ 8t u <t hu u lu t s t  Z|u @ s @ s u s u s u s u s u s u s u  t u t u t u t u t s     @u D$d+    S  1[^_]t& ,t t& \u du `u 5u =u +5 t =$t  E+(t RPD$D$T$l$+5t D$=t +t T$RQ|$D$|$$T$ |$    HH9    }(4$|$-t 5t =t ]&    v  t t$$t 5(t WVD$D$T$ t D$$t (t XZ&    &    1u&    v 1       &    UWVSD$T$$@u %  hu lu D$Du T$+Pu Tu Hu 5Hu !Lu T$T$!=Lu     !!	Xu DE|$%Xu \u `u du 1 EED$T$u 9t<|$u u W<$u u W}u u W;5u u },@u 9e[^_]v &    fu u |$ 6eWfUWVS   d    $                      KTCP        ?B 9q    i  sH{Ll$D$t$|$e  D$T$D$D$T$D$D$ut=d      t-u'D$T$L$&    l$  1|$        @u $=u D$ 1|$PL$D$Z9t7 ;u    u u u Du u    u u 	t+u u u u $    @u d      t tu@  u~   $   d+          [^_]t& l$&    ;9       tsH{L t& D$ @  t   5	v1jt&    $tT@ksX{\  $9Jv         t"ffi     v WVSv 1v v   vdMbi  )      v [v +v v v ^=v v _v t& 11&    v          1v  @v       $   $ v v v     v     v v   fv v U]&    t& =v t&     tU V5 S}%9   | ;i ;[^]v WVSv d    D$1       u 0  1   = $    v v 5v =v $ T$H    t)5v 5v $T$=v v v t5       v     1v v D$d+      [^_v 8       1 v     t&  @  $ &    v   t         v 5v v 1    v     `  11	F  t&   )  11	  &     1 v    v      L$v    QQ  L$)YQ   1 v     v 5v v     D$Q v    PT$)Q  5 ^= &    &    WVSd    D$1 @uv tD$d+    `  [^_t& $\$D$ eRPD$L$D$T$[^?KL v] ~N;wO}%9      i ; ; /hY j `v j j Fv     tk    <)T$T$1D$D$    }%9   |Q ;i ; cC   ` /hY O    .hY   :&    &     @uv t             &    t& UWVS,L$|$@d    D$(1 $  v D$D$   D$T$    t$Cv Sv D$T$=v $@     T$D|$ ID$iIy   s{CC C8   C C$$C<    C( C@  C0C4    CD    CXD$C\    8C`    Cd       Ch    Cl    Cp    Cx    C|                                                            E UD$CHSLET$CPCTD$uuMb)SPSTU E5 - T$9  D$(d+    C  ,[^_]    v v L$    $5v =v     t$L$|$  iL$   y  D$ts{fMbC    St& Cv D$D$T$GPD$WTGXG\1&    D$ v w Gv WGG$D$ GD    G0G4    G@   t$    $   $%    D$t^  cis  D$  T$1t$;T$t$MT$^MD$T$     MT$Lv v tC tC   t%C0t$PD
   9O1H tS0 9    C4     5v v t$ @tSX    @    L$ v GOv T$OL$$WO(O,L$    GL    W8W<OHt& |$   T$   $ GT$      D$ CHSL&    $$$D$     Ct& $T$Cu9L@B 9Oi   29M e9O+v $t$T$ x  $v  v     ,  u     L$$@T$L$  D$QL$D$T$ X H   $9O   + 1 EET$T$$D$v v   $L$19    MM^     ML  v T$t$ v 15 T$$t$    v t$Zv gt& C(      u&v     @     4$$0   	$ &    D$oD$    T$11	   Dtv  v $C(&    v $EGT$      D$CHSL&    D$>v D$$D$    D$    D$ % v D$$v $    D$    D$    D$ ffffv v &    U    WVSt$4D$T$1   1    D$    T$D$D$0$D$	v ,$tV$D$1T$ EED$T$1t11 EE	u|$$D$[^_]t& 	J	u6UWVS(w     =     x=    dw t9   t
GD    tx    @w       t= X= tZt9dw t?C,@w u.CD u#  9dw tE  [^_]v C0X= u&    W0z 6_  &    h6  jj hI = w&    f&    &    UW w VS5w $   $    w     B &  $1v @Du9uutQ89P8~   P0B u   w    9t%x Bx tfP``DBx u5v u\tXx =x tLj 1B w j     Z    Y  w     tw v     w [^_]t&  B t4$_t& 5w Mv $U WVS = tX1   t& tCD t#    s,)hN  HP7PC0X= u       )H hR  SW[^_]   1&    S dw p,hT  h   S [v UWVS(v     v        5` Q  v                       G          9u    ;    2  =v        9Gbf9vFv 9tG    1B    9w    u        9w9u&    v v !  D$$hv 5    Pu,hD     D$D$D$D$    D$        $v $9p  <$9|$$tU D$T$6 j|$U }uT$$v D$ v ;D$\$Y   L$\$ 9   L$\$ +L$\$!!    \$!!	t$EE1UeM EEt$9\$L|$|$L9|$    LMD$|$D$&    <$=v v L$v \$ 9}u!$v u}f   d       v }  v   ([^_]   d    tju,hC [^([^_]    v v v &    t& fv T&    t& ` u&    t& VS5`w t
`w      X= u\&    C0X= tH9uuCDtCP  KT  `w t@89C8~C0`w X= u[^&        WVS;w    9dw    ;`w     w w 9tCDu,s0K4C0N1C0C4 w 1[^_t& {`sdK`w>K`Kdv ttx =x uw $$v            ;w ;v    ~9dw t;`w +&    v    FS S0C09t [t& 1 [&    PDH0@PDP09t5PXt(w u&        h     v @8    &    U w WVSX-v {  v D$Lx h=x *  l$@1&    v 5(w   D$@@`x|$@=x   D$@@D@$u&    v w D$$D$@T$(D$Dw T$HD$w T$D$,T$0-w D$+D$$T$u}T$(!L$!D$\$#D$\$4!	C  $  9      D$@@D$Q  v C  \$@=w Cpw|$StD$D$$+D$T$ T$(T$ !l$t$!|$!!	WD$DEET$4Wt${sll$8k[hT$T$Ht$T)\$P!!T$0T$0!!\$@	T$DD$$EEKkShT$(L$,L$HCpv KlStL$8D$L$d$L$4 t1D$L$,T$1|$ EE1D$T$W S \$+\$t$t$ HH9  D$@9dw   $!Jw @D;4$|$@ GD(w 9=dw      GDv     h         UEMT$1D$8D$T$<eL$ t1 D$9T$rL$,\$0+L$\$\$4!|$!!!	u<t$8L$<1L$ EEP  9'  $$;    V],4$t$t$SPhF D$X>|$Ltv d        ;    sMw    X w [^_]$|$@T$(GDD$$WtT$HGpD$DWlGh        tw u,P\GD$$!:w   w [],WVSPhE t$$t$$D$\p,hPF $|$@_,SPhG w ppt$4t$4t$Dt$Dt$<t$<p,hG 4wwt$\t$\t$Xt$Xt$(t$(w,hG dw (9*  `  ],4$SPhE dw UWVStPD@  @  7   w 1x Jxx u.   t   ADG`W``    AD@tY`Ad CA4Q`QdQ0B A0   A4"  PA8    = u"&    B0P= t	B8yE Q0   PA0i4U ADI&    fv tw t%x =x t w [^_]    w v     ` &    v  lu 1v 1 1t& UWV S w V0F4B F0   F4"  ^8= tjP &    fB0P= t;Z8~V0PF0 w ~411p1 [^_] t& VS w CD@uS0C09t
S`C`9t[ w ^t& x x BS`Cdx &    &    S`w t+9t;PHtu`w  w $w [f=    u  [ w [$w fUWVSD$`w   9    w $w 9r5`w 11D$D$9D$tPLt[^_]    =`w |$)|$\$wK[!!$L$\$$!!	u~L$d$1 EEg    +L$1d$ EEA&    f w $w 9r<11[^_]t& L$1d$1 EEt& |$|$v  = t'SXv SPtC4X= u[v &     S= t!Xv STtC0X= uv [t& v v UWVSu}D$T$D$    $t
1D$$T$9CCD$)1L$ MEEt]3{[^_]&    UWVS  @   F P^   1$tQkd)$d   F  nQ&    f   1^n$tQkd)$d   Fr9rFV$F$T$Pt$t$RFPVv(v$t$ t$ v,hI 4[^_]@V1$T$t11   	t/|$    X  9       iX  &    v F^$ ;R1PT$F Y[]u&1 ;@ =? FF Gt& =? 70r*n&    14v f=    uv,hXI    XNn^Zy&    UWVS{<  C@    w  P=    {8     B0P= t9z8}S0PC0C`s4SDC`Cd}   x x BcDS`Cdx  w 161CDt-sP  KT  `w t@89C8~`w  1[^_]v t SDv C<     8UWVSY   |
turNj
L)L)r$11D5 9r$ [^_]t& t
tLfL    tf&    fVS$d    T$ 1t$0xD  X= tC,t5C0X= u D$ d+    u%$[^v  D&    fS L$@w x11 [fU    1]&    &    UWVt&     u5    =       9u^_]t& U   MbWw V S  w   w ))1t11  ;1t11   w w 1[^_]ffff&    v WVSXDBt\C ~6	t9    ;    r{ u4C	u[^_&    CSv 1[^_&    D$T$tL$t& D$PRXZfUWVS<D$PT$8T$TD$,        T$0D$4Qh  U8D$D$(    ,$&    t$(h  |$W]t$Vh  WMvh  W?$5    h  W+vh(  Wv$v h<  W(hV  WD$4F D$T$8V$11T$ND$XZ|$   t& VN~T$VL$ NT$T$L$N$L$'WVShf  \$S~|$;Whw  Sm h  \$S[L$$\$(+D$T$RPt$$|$(+D$T$ RPS1QWVhDJ t$,|$40wD$)!t9r|$T$D$(D$(D$(@,$t$4v$v h  h  UF1RPh  h  UxF1(RPh  h  U^F1RPh  h  UFF1(RPh  h  U+F1RPh  h  UD$`(1@,RPh  h  Us8s4h  h  UC01(RPh  h  UCD1RPh  h  UCH1(RPh  h  UCL1RPh$  h  Uq(sTsPh0  h  UXs\sXh?  h  UB(sds`hM  h  U)slshh[  h  U(stsphj  h  UCx1RPh{  h  U(      h  h  U      h  h  U    1(RPh  Ul$`D$d  L[^_]y&    fWVS|$Qh  PT  Wh  S=h  S/XZu  v`h  Svvh"  Sv vh9  S,v$hP  Sv(he  Sv,hz  S$vvh  Svh  SFDtPh  SF8tPh  SzF<tPh  SdF@tPh  SNFHtPh  S86h0  S(h  Sv4hF  S yh  SXZ[^_&      tP`Rh\  St& h  SYX|    h  SY[[^_&    &    SHjP5    ho  S^P5    hxJ SG$h  S9XZ[t& WVS2Bt;tHVX1[^_t& rr1ZY[^_tE1[^_&    rzh  Sjh  SWVh  S$h  S1[^[^_&    SXD    Q ;    r{ uC[1fWVSh  j #jh  j WVh  j $h  j [X&    W1VZY    ;    r1)&    HS1,X    ;    r[^_UWVS(d    L$$1t$\$<Q VY|$  v Q x   Si  )Sk<)$IN)    )L$)BHSb    D$T$ Qj h: D$(T$,kda, D 1l$T$O[d   T$D$D$T$T$t$`A^1D$T$   D$iD$]  1 xt$4$iqz  xvml  L$SsCKD$$d+    uI([^_]&    Q Q e&    v D$Yv L$;VSCD$ST$C1SN1C     C     CS[^t& UWVS 3{$+CT$S#F#NfNKCS !!{ |$KN1{ EE4$CSCsSe[^_]&    v UWVS0s{$t$ q|$$yt$(t$ |$,|$$)T$D$({#T$,!s|$t$s [t$\$YIL$L$(\$,9s`D$(T$ +$L$$L$!T$,!L$+D$T$1 EED$T$)e[^_]&    L$D$T$1 EED$T$e[^_]fffffUWVS    t)&    fC+t$Xu[1^_]&        UWVS    t$|$tv CWVXZu[1^_]UWVS    t$|$tv CWVXZu[1^_]UWVS    t$|$tv CWVXZu[1^_]S x x  x [&    v UWVpx@@@x )^_]fUSP`@\)]t& &    UWVSS@d    E1UU    CDUtoMU   tCMUJ0`   fJ4Mj<B8    HMpxj uR   j jSEd+    ue[^_]t& u&    t& UWVS^@d    E1    FD]   MU$   tXMUJ0`   j<fJ4B8    xO@HO_HMX]HMXj uR$   j jVEd+    ue[^_]v s&    WVSd    D$C$  u@   tT$,pxPD$d+    u[^_    ut&    fVSd    D$F$  uS    t*XS@PK[HL$(X\$,HXD$d+    u[^t& ut&    fUWVSx   t[^_]v Mu  }h   C@WVPhJ S`  [^_]UWVSx   t[^_]t& MUu}L$M$  h  C@WVt$t$PuhJ S`  ,[^_]W1Vt$WVXZ^_&    t& 1&    fW1VRSx @{<s@C0 l CD    [^_t& W|$wUWVSEEUWEFUV)x?;u}       }Auu]CSXZe[^_]g_v e11[^_]g_t& uxQuuEUE]CSCSYM[]9| g    UWVS@@@x WVY[e[^_]    UPWVpxQRY^e^_]&    fPH@H    t1v U   ]fW|$wUWVS,d    E   EEd    E3CE{C  Rr$z(B,RPEEUESCXZEd+    ue[^_]g_&    U1]t& UWVS8x d    D$41        C$@xtp   tf|$1   \$D$D$  D$= w   F x x t- x D$4d+    uf8[^_]t&   $$tx 11$t&  x f&    ft& UWVSx@x T$$Cyw$x >t& [^_]&    d        sd        tIRPXZd    u&    CDtT$x cDiS    x[Pv UWVSs,$vx D$C0tD$$x c0k(   vx $tACSl$CSx CSC0   $L$   K0L$vx >t& [^_]&    d        sd        tIRPT$XZd    ut& |f$L$L$H&    fUWVS@@4@x x SKj Cj S j K$S(K,CDuKD[^_]t& t& UWVSh@Tm x T$$T$sD${x CD   sKDCSj j j T$D$Dm x >t& [^_]d        sd        tIRPXZd    u&    V&    v WVS@@@x RPY^[^_t& UWVS|$ l$u&   @x RPXZts\{`[^_]&    CP[^_]X&    W|$wUWVSd5    <U?Md    E1sH&       FUMCHtt& xCHt   utt& d    @    [H          d5    D     x M])19    |Ud+       e[^_]g_EEUEEEEE8  1j   t9=    PWh     ZY5&    _x    @M])Hx M]MPx Tx 9U|	u(EU5`x Px ETx UXx \x Hx x    {&    v S x x  x t x 1[&    v UhPWVSp<x@   @x WVC$XS(C\K`Z[^_]&    &    V x S5x  x t        C    C1[^    WVS1t1	\ x =x  x t[x 1[^_t& UVS1t1	\ x 5x  x t[[^x ][^]v WVS x 5x  x tu#   t`C1t	t vCd1x {P{P1    l    x           [^_t& &    v UWVS1XD$d    D$T1t1	\d    $ x =x  x   B  #   @  [1x D$N}%D$D$8 l D$D$D i v9   \$HD$L        ;i ;1ty[x WVSD$Y=tT$Td+       X[^_]    $@  L  P  <  ]  &    d      @=    uRSD$Z=Dt& ;g    W    PD$Y/&    fUx WVSHD$d    D$DHx Px Tx Px     Tx     `x $Xx \$\x D$Hx L$\$ x =x  x |$D$      ]tDs{E)$T$	t9}CS4$|$D$D$T$D$|$   l$<$	  5w9      >t& t$ x T$ t$@t$@t$@t$@t$@t$@t$@t$@t$@$j j D$T$R x P   D$Dd+       H[^_]v D$   v d        Qd        t@t$T$L$Xd    &    f1s&    fD$1  UD$1  ;&    t& UhWxVSC    CCtDCPS1tSC$	T$u#[^_]&    1t& C@   @x t$t$C   CXSZCffff        B    B1v USP`@\)]t& &    S@C<    C@    P4xC     1[       [P1P1&    t& UWSXPp@p<Y[e[_]&        Pv U}%WVSyq9   |+ ;i ;1SZ[^_]v t& U   ]&        U   ]&        U1]US d    D$   $D$CD$CD$d+    u]1t& U1S@B  d    D$1$D$CD$CD$d+    u]1&    fUWVS0K@sT$S<d    D$,1C8D$	uwC u|$uGCV$T$L$v0$1|$9   |$tD$@   D$,d+       e[^_]&    fD$D$L$D$AD$ACV$T$C4u|$u$L$~,C$S(KfL$v0$19TD$|$GD$GD$G7t& UWVSh$T$ tC<    C@    U4      C4C     C,    C0    C4FF   }%N~9       ;i ;|$D$    D$T$S@}%C<FNv9       ;i ;1<$uRs81QW}8X1ZC 1[^_]&    F<[^_]t& d      I    tsQYv /&    fO1U]fWVSd    D$d      @px3@ {CRPD$D$T$D$SCXZD$d+    u	1[^_t& 11fd &    t& Su$CHCL    [v x KCtHC"  x fUWVSSt$ D$$D$CP$D$u`    DC$L$Cl uCp@RPXZ|$ {\k`{dkht[^_]t& j j j D$[^_]fWVS1xwd      1iGax uAv Ct7t0;PDu;xuxd      9CDt1[^_t& [^_t& UWVShxCU<t[^_]    t& VSd    T$tW11rCt6H$t/1$CD$d+    u[^t& &    t& UWVSd    D$1L$ t1rT$$?$u4dt& pt.QXu!T$1t2CuT$CD$d+    u[^_]t& W|$wUWVS,d    E   EEd    E3CE{C  Rr$z(B,RPEEUESCXZEd+    ue1[^_]g_    UhPWVSp<x@@p@WVC$CdS(Shj K\[`)SQ[^_]&    U}%WVSyq9   |[ ;i ;1uSZ[^_]d      I    tQY    t& UWVS$D$T$L$d    D$ 1  |$  !  D$D$  P       EL  @    P1@    )J 19rE    d      l$CT$ED$fx sT1iGa<x ,x   t& @    ;XDuP9u    HST9$tx tl$v l$x   t$  D$|$uE$ED$E(E  d5        G(       '       _  P	!  ]HEH  |$GWE8ELPELP ELU   PELT$@D$  D$P      d        ED  PXjU PXUhX     1:D$@1t$  9  uD$@$?1D$xhD$x tD$GsT1D$D$ d+      $[^_]ELE8    @   EL@     ELUP d        t      P	xu]Hi        DD$U   WT >   (      z   i   1    WVSd    D$d      @px3@ {CRPD$D$T$D$SCXZD$d+    u	1[^_WVSd    D$d      @px3@ {CRPD$D$T$D$SCXZD$d+    u	1[^_UWxVSSSC       CCS1B$CSC1~`CCtXC @?B $CT$S9|$@B T$ R   PT$L$CXSZCC   [^_]&    1_&    fV   S   d    $   1umxq   !  tvHtvy $   d+    u^   [^t&    t            E&    f&        WVS   d    $   t)$   d+          [^_&    fxLg!  tUHtXxE&    v         E&    f]S&    UWVSd    D$Ct@@E<u $ED$d+    ua[^_]C9E4uEP(E$U(E    E4E,U0E$E(s19LsfHLQ$1PH@8@L&    VSDpd    T$@Pt?@   tT$@d+    u8D[^t& v D$@d+    uD1[^c&    &    SX d    T$1>tT$d+    u![&    t&    SX d    T$1tT$d+    u![&    t&    S d    T$t3P,19X0M$D$d+    u[UWVS8Xhd    T$418Pt$1E$tZD$uD|$;D$w?D$x9|$;w/4$L$_ZuttfT$4d+    u8[^_]UWVS8Xhd    T$418Pt$1E$tZD$uD|$;D$w?D$x9|$;w/4$L$ZuttfT$4d+    u8[^_]VS d    T$   C}   P tvtrd5      CB   C"    CD    $C   81T$d+    u0[^v t& ]t& t& UhXWVS@X9tfv _XsP t!uCuCB   C"     |GX9u[^_]VSd    T$1PxPwk!4  tYFtRuWNT$d+    uE[^&            E&    f&    ft& SX d    T$1xYwl!  tZJtT$d+    uF[&    v tt&         Et& Sx,?!  t-Jt-[            E[[fP 1SX d    T$1xYwl!  tZ
uuT$d+    uC[&    tt&         Et& VSd    T$1PxPwk!4  tYFtRuWNT$d+    uE[^&            E&    f&    ft& SX d    T$1xYwl!  tZJtT$d+    uF[&    v tt&         Et& P SX d    T$1xYwl!  tZ
uuT$d+    uC[&    tt&         Et& UWVSpd    T$1xP   wo!,  t]M      D$x;|$;w1   ui1d    D  H  uT$d+    uL[^_]            Ee    11&    v &        UWVSpd    T$1xP   ww!,  teM      D$xC|$;w9tu   um   d    D  H  u    T$d+    uD[^_]            E]    11fffffffUWVS K<s\{`L$K@9|$L$L$t$|$t$   L$|$   |$+D$T$D$)T$9    s4\$&    D$T$)9r\$xsv D$T$D$D$T$T$9rED$T$D$T$D$T$C\   S`1 t1C$S()t$|$sD$T$e[^_]&    UWVS$R	tD|$9D$rFk{	t;k>t& $D$    S	u[^_]&    ;^sV[^_]&    v $    d        sd        tT$ @t$t$XZd    `VfWVSX
$  u  
   Op\X`PPHdt;7Gs7_[^_&    WVStPu d    tN  [^_&    v tu?td    ;  tGu1[^_      [^  _    1td      9  t1&    t& VS1't7    F    tF@B 1[^t& F   1[^VSF1tRFPF    FP   t(P	x^h1[^t&    f       @o&    &    @O&    &    UWVST$D$    D$t$_D$D$   Ct$S9$rR   CC$   tC    ESUkC _tASCtL$9$sGD$D$Wa[^_]t& &    t& S1*t[[&    &    VSd    D$11ChCtc   Ctt6  $   T$d+       [^&    Cdu)  $Ch1&    v SPSCd    uT  K\   S`33P	u     $@      v Sdt)wf$  P3K\3S`	JCP9CPuH&    &    Sd      t    1C    C   [[&    v Sd        t    1C    C   [[t& UWVS`D$$  d    T$\1|$      uhT$(D$0T$4D$HD$8T$LT$<L$(\$,D$PL$@\$DT$TD$$TDD@L$\d+      e[^_]          \$   \$   \$   L$L$D$L$T$L$\$t$H|$LD$PL$@\$DT$TYt& \$      D$   T$&    v T$(D$0|$D$ D$4D$\$ L$13D$	t;D$ D$D$rD$(|$D$ D$,D$&    \$ L$13D$	t;D$ D$D$rD$8|$D$ D$<D$&    \$ L$13D$	t;D$ D$D$rD$      nt& UWVS0d    T$,1D$|$D$   N1   t1|$       D$tbt11t& D$1D$D$D$CD$C1T$,d+    u3e[^_]t&     ft& WVS{d    D$11ChCtht[Ct"t}    t]t11   Ip  $D$d+    u-[^_&    f    f&    &    S@8t]CLtNC@C<u1C\    C`    [t& C4PS4tC4[&    CHC\    C`    [UWVS`ET$L$ D$8d    D$\F1FhFD$<  \$ }%CK[9      ;i ;1D$T$D$0T$4D$<T$X-  F<V@$F\T$D$F`D$FtD$$C  FdtVPFd    F  T$<   D$(T$,D$8t#|$|$  @    @    @    T$$  T$4D$0	  D$uL$(\$,D$0T$4D$T$D$T$L$0\$4F\D$(V`T$,9sT$<D$<T$X  \$ }%K[9   S   ;i ;1L$(\$,F<F4V@T$4F,    F0    F$F4D$0F(9r9	d  D$<T$XtFd  D$<T$X  |$8t$$L$D$@D$@D$DGD$HGD$\d+      D$$e[^_]&    D$<      T$,D$(   D$(    D$,    &    F\    F`    t& L$,T$(L$(\$,9r!D$8@   @    @    v )D$@D$@|$8GD$DGD$HG&    ft& D$<    L$(\$,8v D$$    D$0D$4D$D$v FtD$$   t& &    fVt$<u<          @    t& D$(T$,{    $  fUWV)   S   $l$ d    $   1\$8D$DD$Hd=    |$ht"$   d+         [^_]D$    D$    D$FD$    D$FD$D$0j T$L$Y     uET$|$   u2	   F   D$0D$0   tD$|$    $D$T$1PZuzD$0L$L$D$		<$l$d    L  8  P  D  tST$t& D$0xD$0t    YD$0i_&    t& UWVSd-    t!$tX11$9tB=t[^_]&    u"@  <   [^_]v fUS Xd    T$1P@$   L$D$D$D$D$D$	T$d+    u]fVSd5    =t[^t& u@  <   [^t& UWVS0S`d    D$,CT$D$C\D$1ChC   K@S<D$D$L$D$FD$FD$	tmD$D$Ct|           |$   t[T$L$D$D$FD$FD$FfD$,d+    |   e[^_]v D$1D$T$9r/F   F    F    &    &    v )L$\$[v & 